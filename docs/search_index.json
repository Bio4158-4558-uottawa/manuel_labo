[
["index.html", "BIO4558 Biostatistiques appliquées avec R Manuel de Laboratoire Note", " BIO4558 Biostatistiques appliquées avec R Manuel de Laboratoire Julien Martin 04-09-2020 Note Version en cours de développement pour le cours de l’automne 2020. Les chapitres vont apparaitre au cours de la session. "],
["préface.html", "Préface Quelques points importants à retenir Qu’est-ce que R et pourquoi l’utiliser dans ce cours? Installation des logiciels nécessaires Instructions générales pour les laboratoires Notes sur le manuel", " Préface Les exercices de laboratoire que vous retrouverez dans les pages qui suivent sont conçus de manière à vous permettre de développer une expérience pratique en analyse de données à l’aide d’un logiciel (R). R est un logiciel très puissant, mais comme tous les logiciels, il a des limites. En particulier il ne peut réfléchir à votre place, vous dire si l’analyse que vous tentez d’effectuer est appropriée ou sensée, ou interpréter biologiquement les résultats. Quelques points importants à retenir Avant de commencer une analyse statistique, il faut d’abord vous familiariser son fonctionnement. Cela ne veut pas dire que vous devez connaître les outils mathématiques qui la sous-tendent, mais vous devriez au moins comprendre les principes utilisés lors de cette analyse. Avant de faire un exercice de laboratoire, lisez donc la section correspondante dans les notes de cours. Sans cette lecture préalable, il est très probable que les résultats produits par le logiciel, même si l’analyse a été effectuée correctement, seront indéchiffrables. Les laboratoires sont conçus pour compléter les cours théoriques et vice versa. À cause des contraintes d’horaires, il se pourrait que le cours et le laboratoire ne soient pas parfaitement synchronisés. N’hésitez donc pas à poser des questions sur le labo en classe ou des questions théoriques au laboratoire. Travaillez sur les exercices de laboratoire à votre propre rythme. Certains exercices prennent beaucoup moins de temps que d’autres et il n’est pas nécessaire de compléter un exercice par séance de laboratoire. En fait deux séances de laboratoire sont prévues pour certains des exercices. Même si vous n’êtes pas notés sur les exercices de laboratoire, soyez conscient que ces exercices sont essentiels. Si vous ne les faites pas, il est très peu probable que vous serez capable de compléter les devoirs et le projet de session. Prenez donc ces exercices de laboratoire au sérieux ! Les 2 premier laboratoires sont conçu pour vous permettre d’acquérir ou de réviser le minimum de connaissances requises pour vous permettre de réaliser les exercices de laboratoires avec R. Il y a presque toujours de multiples façons de faire les choses avec R et vous ne trouverez ici que des méthodes simples. Ceux et celles d’entre vous qui y sont enclins pourront trouver en ligne des instructions plus détaillées et complexes. En particulier, je vous conseille : R pour les débutants http://cran.r-project.org/doc/contrib/Paradis-rdebuts_fr.pdf An introduction to R http://cran.r-project.org/doc/manuals/R-intro.html Si vous préférez des manuels, le site web de CRAN en garde une liste commentée à : http://www.r-project.org/doc/bib/R-books.html Une liste impressionnante de très bon livre sur R https://www.bigbookofr.com/ Finalement, comme aide-mémoire à garder sous la main, je vous recommande R reference card par Tom Short http://cran.r-project.org/doc/contrib/Short-refcard.pdf Qu’est-ce que R et pourquoi l’utiliser dans ce cours? R est un logiciel libre et multiplateforme formant un système statistique et graphique. R est également un langage de programmation spécialisé pour les statistiques. C’est un dialecte du langage S. S-Plus est un autre dialecte, très semblable, et forme un produit commercial qui a un interface graphique que certains trouvent plus convivial. R a deux très grands avantages pour ce cours, et un inconvénient embêtant initialement mais qui vous forcera à acquérir des excellentes habitudes de travail. Le premier avantage est que vous pouvez tous l’installer sur votre (ou vos) ordinateurs personnel gratuitement. C’est important parce que c’est à l’usage que vous apprendrez et maîtriserez réellement les biostatistiques et cela implique que vous devez avoir un accès facile et illimité à un logiciel statistique. Le deuxième avantage est que R peut tout faire en statistiques. R est conçu pour être extensible et est devenu l’outil de prédilection des statisticiens mondialement. La question n’est plus : \" Est-ce que R peut faire ceci? “, mais devient” Comment faire ceci avec R \". Et la recherche internet est votre ami. Aucun autre logiciel n’offre ces deux avantages. L’inconvénient embêtant initialement est que l’on doit opérer R en tapant des instructions (ou en copiant des sections de code) plutôt qu’en utilisant des menus et en cliquant sur différentes options. Si on ne sait pas quelle commande taper, rien ne se passe. Ce n’est donc pas facile d’utilisation à priori. Cependant, il est possible d’apprendre rapidement à faire certaines des opérations de base (ouvrir un fichier de données, faire un graphique pour examiner ces données, effectuer un test statistique simple). Et une fois que l’on comprend le principe de la chose, on peut assez facilement trouver sur le web des exemples d’analyses ou de graphiques plus complexes et adapter le code à nos propres besoins. C’est ce que vous ferez dans le premier laboratoire pour vous familiariser avec R. Pourquoi cet inconvénient est-il d’une certaine façon un avantage? Parce que vous allez sauver du temps en fin de compte. Garanti. Croyez-moi, on ne fait jamais une analyse une seule fois. En cours de route, on découvre des erreurs d’entrée de données, ou que l’on doit faire l’analyse séparément pour des sous-groupes, ou on obtient des données supplémentaires, ou on fait une erreur. On doit alors recommencer l’analyse. Avec une interface graphique et des menus, cela implique recommencer à cliquer ici, entre des paramètres dans des boîtes et sélectionner des boutons. Chaque fois avec possibilité d’erreur. Avec une série de commandes écrites, il suffit de corriger ce qui doit l’être puis de copier-coller l’ensemble pour répéter instantanément. Et vous avez la possibilité de parfaitement documenter ce que vous avez fait. C’est comme cela que les professionnels travaillent et offrent une assurance de qualité de leurs résultats. Installation des logiciels nécessaires R Pour installer R sur un nouvel ordinateur, allez au site http://cran.r-project.org/. Vous y trouverez des versions compilées (binaries) ou non (sources) pour votre système d’exploitation de prédilection (Windows, MacOS, Linux). Note : R a déjà été installé sur les ordinateurs du laboratoire (la version pourrait être un peu plus ancienne, mais cela devrait être sans conséquences). Rstudio RStudio est un environnement de développement intégré (IDE) créé spécifiquement pour travailler avec R. Sa popularité connaît une progression foudroyante depuis 2014. Il permet de consulter dans une interface conviviale ses fichiers de script, la ligne de commande R, les rubriques d’aide, les graphiques, etc. RStudio est disponible à l’identique pour les plateformes Windows, OS X et Linux. Pour une utilisation locale sur son poste de travail, on installera la version libre (Open Source) de RStudio Desktop depuis le site https://www.rstudio.com/products/rstudio/download/ Paquets pour R Rmarkdown tinytex Ces 2 paquets devrait être installé automatiquement avec RStudio, mais pas toujours. Je vous recommande donc de les installer manuellement. Pour ce faire, simplement copier-coller le texte suivant dans le terminal R. install.packages(c(&quot;rmarkdown&quot;, &quot;tinytex&quot;)) Instructions générales pour les laboratoires Apporter une clé USB ou son équivalent à chaque séance de laboratoire pour sauvegarder votre travail. Lire l’exercice de laboratoire AVANT la séance, lire le code R correspondant et préparer vos questions sur le code. Durant les pré-labs, écouter les instructions et posez vos questions au moment approprié. Faites les exercices du manuel de laboratoire à votre rythme, en équipe, puis je vous recommande de commencer (compléter?) le devoir. Profitez de la présence du démonstrateur et du prof… Pendant vos analyses, copiez-collez des fragments de sorties de R dans un document (par exemple dans votre traitement de texte favori) et annotez abondamment. Ne tapez pas directement vos commandes dans R mais plutôt dans un script. Vous pourrez ainsi refaire le labo instantanément, récupérer des fragments de code, ou plus facilement identifier les erreurs dans vos analyses. Créez votre propre librairie de fragments de codes (snippets). Annotez-là abondamment. Vous vous en féliciterez plus tard. Notes sur le manuel Vous trouverez dans le manuel des explications sur la théorie, du code R, des explications sur R et des exercises. Le manuel essaie aussi de mettre en évidence le texte de différentes manières. Avec des sections à vous de jouer, ui indique un exercise à faire, idéalement sans regarder la solution qui se trouve plus bas. des avertissements des avertissements des points importants des notes et des conseils "],
["introductionR.html", "1 Introduction à R 1.1 Paquets et données requises pour le labo 1.2 Importer et exporter des données 1.3 Examen préliminaire des données 1.4 Créer des sous-ensembles de cas 1.5 Transformations de données 1.6 Exercice sur R", " 1 Introduction à R Après avoir complété cet exercice de laboratoire, vous pourrez : - Ouvrir des fichiers de données R déjà existants - Importer des ensembles de données rectangulaires - Exporter des donnes de R vers un fichier texte - Vérifier si les données ont été correctement importées - Examiner la distribution des observations d’une variable - Examiner visuellement et tester la normalité d’une variable - Calculer des statistiques descriptives d’une variable - Effectuer des transformations de données 1.1 Paquets et données requises pour le labo Ce laboratoire nécessite: les paquets R: car lmtest boot lmPerm les fichiers de données sturgeon.csv 1.2 Importer et exporter des données Il existe de multiple format four sauvegarder les données, les 2 plus utiles sont .csv et .Rdata. Les fichiers .csv sont utilisés pour stocker des données. Ils sont ouvrables par les éditeurs de texte (e.g. Word, Writer, atom, …) et les tableurs (e.g. MS Excel, LO Calc). Les fichiers .Rdata sont utilisés pour stocker n’importe quelle objet R pas uniquement des données. Cependant, ces fichiers ne peuvent être lus et utilisés que par R. Les données pour les exercices de laboratoire et pour les devoirs vous sont fournies déjà en format .csv. 1.2.1 Ouvrir un fichier de données en format .Rdata Pour ouvrir ces fichiers, vous pouvez cliquer dessus et laisser votre système d’exploitation démarrer une nouvelle session de R ou encore, à partir de la console de R, taper sur une ligne de commande : load(file.choose()) ce qui ouvrira une boîte de dialogue vous permettant d’aller choisir un fichier sur votre ordinateur. Si cette option semble très attirante de part sa simplicité, je ne recommande pas de s’en servir car elle ne permet pas de reproduire l’analyse facilement. En effet, elle nécessite de choisir le document chaque que l’on souhaite l’utiliser. Vous pouvez aussi directement taper le nom du fichier entre guillemet \"nom du fichier avec l'extension\". Par exemple, load(&quot;ErablesGatineau.Rdata&quot;) 1.2.2 Ouvrir un fichier de données en format .csv Pour importer ces données en format .csv dans R, il faut utiliser la commande read.csv(). Par exemple, pour créer un objet R erables qui contient les données du fichier ErablesGatineau.csv, il faut utiliser la commande suivant. erables &lt;- read.csv(&quot;data/ErablesGatineau.csv&quot;) Attention si vous travaillez dans une langue utilisant la virgule au lieu du point décimal. Par défaut, R utilise le point décimal et vous n’obtiendrez pas le résultat escompté. Il existe une version modifiée de read.csv() appelée read.csv2() qui règle ce problème. Googlez-la si vous en avez besoin. Pour vérifier si les données ont bel et bien été lues, vous pouvez lister les objets en mémoire avec la fonction ls() ou en obtenir une liste avec une description plus détaillée avec ls.str(). Je vous déconseille cependant, la fonction ls.str() car elle peut produire des sorties extrèmementn longue si vous avez beaucoup d’objet dans l’environnement R. Je vous suggère donc d’utliser ls() et ensuite str() sur l’objet qui vous intéresse. ls() ## [1] &quot;erables&quot; str(erables) ## &#39;data.frame&#39;: 100 obs. of 3 variables: ## $ station: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ diam : num 22.4 36.1 44.4 24.6 17.7 ... ## $ biom : num 732 1171 673 1552 504 ... R confirme avoir en mémoire l’objet erables. erables est un tableau de données rectangulaire (data.frame) contenant 100 observations (lignes) de 3 variables (colonnes): station, une variable de type Facteur avec 2 niveaux, et diam et biom qui sont 2 variables numériques. 1.2.3 Entrer des données R n’est pas un environnement idéal pour entrer des données. C’est possible, mais la syntaxe est lourde et peut inciter à s’arracher les cheveux. Utilisez votre chiffrier préféré pour faire l’entrée de données. Ce sera plus efficace et moins frustrant. 1.2.4 Nettoyer/corriger des données Une autre opération qui peut être frustrante en R. Mon conseil : ne le faites pas là. Retournez au fichier original, faites la correction, puis re-exportez les données vers R. Il est finalement plus simple de refaire exécuter les quelques lignes de code par la machine. Vous aurez à la fin une seule version (corrigée) de vos données et un code qui vous permet de refaire votre analyse. 1.2.5 Exporter des données à partir de R. Vous pouvez utiliser la fonction, {r write, eval =FALSE) write.csv(mydata, file = \"outfilename.csv\", row.names = FALSE) où mydata est le nom du base de données à exporter et outfilename.csv est le nom du fichier à produire. Notez que ce fichier sera créé dans le répertoire de travail (qui peut être changé par le menu à File&gt;Change dir, ou par la commande setwd()) 1.3 Examen préliminaire des données La première étape de toute analyse est l’examen des données. Elle nous permet de découvrir si on a bien importé les données, si les nombres enregistrés sont possibles, si toutes les données ont bien été lues, etc. L’examen préliminaire des données permet souvent aussi d’identifier des observations suspectes, possiblement dûes à des erreurs d’entrée de donnée. Finalement, l’examen graphique préliminaire permet en général de visualiser les tendances principales qui seront confirmées par l’analyse statistique en tant que telle. Le fichier sturgeon.csv contient les données d’une étude effectuée sur les esturgeons de la rivière Saskatchewan. Ces données ont été récoltées, entre autres, pour examiner comment la taille des esturgeons varie entre les sexes (sex), les sites (location), et les années (year). Pour recommencer avec une ardoise vide, videz la mémoire de R de tout son contenu en tapant la commande rm(list=ls()) Chargez les données du fichier sturgeon.csv dans un objet sturgeon. Pour obtenir un aperçu des éléments du fichier qui ont été chargés en mémoire, taper la commande str(sturgeon). sturgeon &lt;- read.csv(&quot;data/sturgeon.csv&quot;) str(sturgeon) ## &#39;data.frame&#39;: 186 obs. of 9 variables: ## $ fklngth : num 37 50.2 28.9 50.2 45.6 ... ## $ totlngth: num 40.7 54.1 31.3 53.1 49.5 ... ## $ drlngth : num 23.6 31.5 17.3 32.3 32.1 ... ## $ rdwght : num 15.95 NA 6.49 NA 29.92 ... ## $ age : int 11 24 7 23 20 23 20 7 23 19 ... ## $ girth : num 40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ... ## $ sex : chr &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; ... ## $ location: chr &quot;THE_PAS&quot; &quot;THE_PAS&quot; &quot;THE_PAS&quot; &quot;THE_PAS&quot; ... ## $ year : int 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ... 1.3.1 Sommaire statistique Pour un sommaire du contenu du base de données appelé sturgeon qui est en mémoire, taper la commande summary(sturgeon) ## fklngth totlngth drlngth rdwght ## Min. :24.96 Min. :28.15 Min. :14.33 Min. : 4.73 ## 1st Qu.:41.00 1st Qu.:43.66 1st Qu.:25.00 1st Qu.:18.09 ## Median :44.06 Median :47.32 Median :27.00 Median :23.10 ## Mean :44.15 Mean :47.45 Mean :27.29 Mean :24.87 ## 3rd Qu.:48.00 3rd Qu.:51.97 3rd Qu.:29.72 3rd Qu.:30.27 ## Max. :66.85 Max. :72.05 Max. :41.93 Max. :93.72 ## NA&#39;s :85 NA&#39;s :13 NA&#39;s :4 ## age girth sex location ## Min. : 7.00 Min. :11.50 Length:186 Length:186 ## 1st Qu.:17.00 1st Qu.:40.00 Class :character Class :character ## Median :20.00 Median :44.00 Mode :character Mode :character ## Mean :20.24 Mean :44.33 ## 3rd Qu.:23.50 3rd Qu.:48.80 ## Max. :55.00 Max. :73.70 ## NA&#39;s :11 NA&#39;s :85 ## year ## Min. :1978 ## 1st Qu.:1979 ## Median :1979 ## Mean :1979 ## 3rd Qu.:1980 ## Max. :1980 ## Pour chaque variable, R donne le minimum, le maximum, la médiane qui est la valeur au milieu de la liste des observations ordonnées (appelée le 50 ième percentile), ici, la 93 ième valeur des 186 observations, les valeurs au premier (25%) et troisième quartile (75%), et si il y a des valeurs manquantes dans la colonne. Notez que plusieurs des variables ont des observations manquantes (NA). Donc, seules les variables fklngth (longueur à la fourche), sex, location et year ont 186 observations. Attention aux valeurs manquantes Plusieurs fonctions de R y réagissent mal et on doit souvent faire les analyses sur des sous- ensembles sans valeur manquante, par des commandes ou des options dans les commandes. On y reviendra, mais prenez l’habitude de noter mentalement si il y a des données manquantes et de vous en rappeler en faisant l’analyse. 1.3.2 Histogramme, densité de probabilité empirique, boxplot et examen visuel de la normalité Examinons maintenant de plus près la distribution de fklngth. La commande hist() permet de tracer un histogramme de la variable fklngth dans le base de données sturgeon. hist(sturgeon$fklngth) Les données semblent suivre approximativement une distribution normale. C’est bon à savoir. Cette syntaxe est un peu lourde puisqu’on doit ajouter le préfixe sturgeon$ devant chaque nom de variable. On pourrait se faciliter la tâche en utilisant la commande attach() mais cela est fortement déconseillé et jamais utilisé dans ce document. Cet histogramme est la représentation classique. Mais les histogrammes ne sont pas parfaits. Leur forme dépend en partie du nombre de catégories utilisées, surtout pour les petits échantillons. On peut faire mieux, particulièrement si on est intéressé à comparer visuellement la distribution des observations à une distribution normale. Mais il faut programmer un peu (ou savoir copier-coller…) Copiez-collez le code suivant dans une nouvelle fenêtre script ( File-&gt;New script, ou Ctrl-n dans Windows), puis exécutez le. library(ggplot2) # use &quot;sturgeon&quot; dataframe to make plot called mygraph # and define x axis as representing fklngth mygraph &lt;- ggplot(sturgeon, aes(x = fklngth)) # add data to the mygraph ggplot mygraph &lt;- mygraph + # add data density smooth geom_density() + # add rug (bars at the bottom of the plot) geom_rug() + # add black semitransparent histogram geom_histogram(aes(y = ..density..), bins = 30, color = &quot;black&quot;, alpha = 0.3) + # add normal curve in red, with mean and sd from fklength stat_function(fun = dnorm, args = list( mean = mean(sturgeon$fklngth), sd = sd(sturgeon$fklngth) ), color = &quot;red&quot;) # display graph mygraph Chaque observation est représentée par une barre sous l’axe des x (rug). En rouge est la distribution normale de données avec la même moyenne et écart-type que les observations. Et l’autre ligne est la densité de probabilité empirique, « lissée » à partir des observations. Si vous êtes plus aventureux, vous pouvez examiner la distribution des observations de fklngth par sous-groupes (par exemple sex et year) avec : mygraph + facet_grid(year ~ sex) Chaque panneau illustre la distribution pour un sexe cette année-là, et la courbe en rouge récurrente représente la distribution normale pour l’ensemble des données. Cette courbe peut servir à mieux évaluer visuellement les différences entre les panneaux. Une autre façon d’évaluer la normalité de données visuellement est de faire un QQ plot avec la paire de commandes qqnorm() et qqline(). qqnorm(sturgeon$fklngth) qqline(sturgeon$fklngth) Des données parfaitement normales suivraient la ligne droite diagonale. Ici, il y a des déviations dans les queues de la distribution, et un peu à droite du centre. Comparez cette représentation à celle des deux graphiques précédents. Vous conviendrez sans doute avec moi qu’il est plus facile de visualiser comment la distribution dévie de la normalité sur les histogrammes et les graphiques de la densité empirique de probabilité que sur les QQ plots. Ceci dit, les QQ plots sont souvent utilisés et vous devriez être capable de les interpréter. De plus, on peut facilement éprouver statistiquement l’hypothèse que les données sont distribuées normalement avec R par la commande shapiro.test() qui calcule une statistique (W) qui est une mesure de la tendance des points d’un QQ plot à former une ligne parfaite. Si oui, alors W=1. Si W s’éloigne de 1 (vers 0), alors les données s’éloignent de la normalité. Ici, shapiro.test(sturgeon$fklngth) ## ## Shapiro-Wilk normality test ## ## data: sturgeon$fklngth ## W = 0.97225, p-value = 0.0009285 W n’est pas très loin de 1, mais suffisamment pour que la différence soit significative. L’examen visuel des grands échantillons est souvent compliqué par le fait que plusieurs points se superposent et qu’il devient plus difficile de bien visualiser la tendance centrale. Les boxplots avec “moustaches” (box and whiskers plots) offrent une alternative intéressante. La commande boxplot() peut produire un boxplot de fklngth pour chaque niveau de sex, et ajoute les coches. boxplot(fklngth ~ sex, data = sturgeon, notch = TRUE) La ligne un peu plus épaisse dans la boîte de la Fig.7 indique la médiane. La coche est proportionnelle à l’incertitude quant à la position de la médiane. On peut visuellement interpréter approximativement les différences entre médianes en examinant si il y a chevauchement entre les coches (ici, il n’y a pas chevauchement, et on conclurait provisoirement que la médiane de fklngth pour les femelles est supérieure à celle des mâles). Les boîtes s’étendent du premier au troisième quartile (du 25ième au 75ième percentile si vous préférez), Les barres (moustaches ou whiskers) au-dessus et en dessous des boîtes s’étendent soit de la valeur minimum à la valeur maximum, ou, si il y a des valeurs extrêmes, de la plus petite à la plus grande valeur à l’intérieur de 1.5x la largeur de l’étendue interquartile . Enfin, les observations qui excèdent les limites des moustaches (donc à plus de 1.5x l’étendue interquartile de chaque côté de la médiane) sont indiquées par des symboles.Ce sont des valeurs qui pourraient être considérées comme extrêmes et possiblement aberrantes. 1.3.3 Diagrammes de dispersion bivariés En plus des graphiques pour chacune des variables séparément, il est très souvent intéressant de jeter un coup d’oeil aux diagrammes de dispersion . La commande plot(y~x) permet de faire le graphique de y sur l’axe vertical (l’ordonnée) en fonction de x sur l’axe horizontal (l’abscisse). Faites un graphique de fklngth en fonction de age avec la commande plot. Vous devriez obtenir: plot(fklngth ~ age, data = sturgeon) R a une fonction qui permet la création des graphiques de dispersion de toutes les paires de variables (pairs()). Une des option de ¬ est l’ajout d’une trace lowess qui indique la tendance de la relation entre les variables. Pour obtenir la matrice de ces graphiques avec la trace lowess pour toutes les variable dans sturgeon, entrer la commande pairs(sturgeon[,1:6], panel=panel.smooth) et vous devriez obtenir pairs(sturgeon[,1:6], panel = panel.smooth) 1.4 Créer des sous-ensembles de cas Il arrive fréquemment qu’une analyse se concentre sur un sous-ensemble des observations contenues dans un fichier de données. Les cas sont d’habitude sélectionnés selon un critère en particulier. Pour utiliser un sous-ensemble de vos données en créant un graphique ou en performant une analyse, on peut utiliser la commande subset(). Par exemple, pour créer un sous ensemble des données du tableau sturgeon qui ne contient que les femelles capturées en 1978, on peut écrire : sturgeon.female.1978 &lt;- subset(sturgeon, sex == &quot;FEMALE&quot; &amp; year == &quot;1978&quot;) sturgeon.female.1978 ## fklngth totlngth drlngth rdwght age girth sex location year ## 2 50.19685 54.13386 31.49606 NA 24 53.5 FEMALE THE_PAS 1978 ## 4 50.19685 53.14961 32.28346 NA 23 52.5 FEMALE THE_PAS 1978 ## 6 49.60630 53.93701 31.10236 35.86 23 54.2 FEMALE THE_PAS 1978 ## 7 47.71654 51.37795 33.97638 33.88 20 48.0 FEMALE THE_PAS 1978 ## 15 48.89764 53.93701 29.92126 35.86 23 52.5 FEMALE THE_PAS 1978 ## 105 46.85039 NA 28.34646 23.90 24 NA FEMALE CUMBERLAND 1978 ## 106 40.74803 NA 24.80315 17.50 18 NA FEMALE CUMBERLAND 1978 ## 107 40.35433 NA 25.59055 20.90 21 NA FEMALE CUMBERLAND 1978 ## 109 43.30709 NA 27.95276 24.10 19 NA FEMALE CUMBERLAND 1978 ## 113 53.54331 NA 33.85827 48.90 20 NA FEMALE CUMBERLAND 1978 ## 114 51.77165 NA 31.49606 35.30 26 NA FEMALE CUMBERLAND 1978 ## 116 45.27559 NA 26.57480 23.70 24 NA FEMALE CUMBERLAND 1978 ## 118 53.14961 NA 32.67717 45.30 25 NA FEMALE CUMBERLAND 1978 ## 119 50.19685 NA 32.08661 33.90 26 NA FEMALE CUMBERLAND 1978 ## 123 49.01575 NA 29.13386 37.50 22 NA FEMALE CUMBERLAND 1978 Dans ces comparaisons, il faut toujours utiliser == pour égal à. Dans ce contexte, si vous utilisez = seulement, vous n’obtiendrez pas ce que vous désirez. Dans le tableau qui suit se trouve une liste de commandes communes que vous allez probablement utiliser pour créer des expressions en R. Operateur Explication Operateur Explication == Égal à != Pas égal à &gt; Plus que &lt; Moins que &gt;= Plus que ou égal à &lt;= Moins que ou égal à &amp; Et vectorisé | Ou vectorisé &amp;&amp; Et contrôle || Ou contrôle ! Pas En utilisant les commandes subset() et hist(), essayez de faire un histogramme pour le sous-ensemble de cas correspondant aux femelles capturées en 1979 et 1980 (donc sex == \"FEMALE\" &amp; (year == \"1979\" | year == \"1980\")) 1.5 Transformations de données Il est très fréquemment nécessaire d’effectuer des transformations mathématiques sur les données brutes pour mieux satisfaire aux conditions d’application de tests statistiques. R étant aussi un langage de programmation complet, il peut donc effectuer les transformations désirées. Les fonctions les plus fréquemment utilisées sont: log() sqrt() ifelse() On peut employer ces fonctions directement dans les lignes de commandes, ou encore créer de nouvelles variables orphelines ou faisant partie d’un data.frame. Par exemple, pour faire un graphique du logarithme décimal de fklngth en fonction de l’âge, on peut écrire plot(log(fklngth)~age, data = sturgeon) Pour créer une variable orpheline (i.e. non incluse dans le data.frame) appelée logfklngth et contenant le logarithme décimal de fklngth, on peut écrire logfklngth &lt;- log10(sturgeon$fklngth) Si on veut ajouter cette variable transformée à un tableau de données (data.frame), alors, on doit préfixer le nom de la variable par le nom du base de données et du symbole $, par exemple, pour ajouter une variable nommée lfkl contenant le log10 de fklngth au tableau sturgeon, on peut écrire: sturgeon$logfkl &lt;- log10(sturgeonfklngth) N’oubliez pas de sauvegarder ce tableau modifié si vous voulez avoir accès à cette nouvelle variable dans le futur. Pour les transformations conditionnelles, on peut utiliser la fonction ifelse(). Par exemple, pour créer une nouvelle variable appelée dummy qui sera égale à 1 pour les mâles et 0 pour les femelles, on peut écrire: sturgeon$dummy &lt;- ifelse(sturgeon$sex == &quot;MALE&quot;, 1, 0) 1.6 Exercice sur R Vous trouverez dans le fichier salmonella, des valeurs numériques du ratio pour deux milieux (IN VITRO et IN VIVO) pour trois souches. Examinez les données pour ratio et faites des graphiques pour évaluer la normalité de la distribution des ratios pour la souche SAUVAGE. "],
["analyse-de-puissance-avec-r-et-gpower.html", "2 Analyse de puissance avec R et G*Power 2.1 La théorie 2.2 Qu’est ce que G*Power? 2.3 Comment utiliser G*Power 2.4 Puissance pour un test de t comparant deux moyennes 2.5 Points à retenir 2.6 Exercice sur la puissance", " 2 Analyse de puissance avec R et G*Power Après avoir complété cet exercice de laboratoire, vous devriez : Pouvoir calculer la puissance d’un test de t avec R et G*Power Pouvoir calculer l’effectif requis pour obtenir la puissance désirée avec un test de t Pouvoir calculer la taille de l’effet détectable par un test de t étant donné l’effectif, la puissance et \\(\\alpha\\) Comprendre comment la puissance change lorsque l’effectif augmente, la taille de l’effet change, ou lorsque \\(\\alpha\\) diminue Comprendre comment la puissance est affectée lorsque l’on passe d’un test bilatéral à un test unilatéral 2.1 La théorie 2.1.1 Qu’est-ce que la puissance? La puissance est la probabilité de rejeter l’hypothèse nulle quand elle est fausse. 2.1.2 Pourquoi faire une analyse de puissance? Évaluer l’évidence L’analyse de puissance effectuée après avoir accepté une hypothèse nulle permet de calculer la probabilité que l’hypothèse nulle soit rejetée si elle était fausse et que la taille de l’effet était d’une valeur donnée. Ce type d’analyse a posteriori est très commun. Planifier de meilleures expériences L’analyse de puissance effectuée avant de réaliser une expérience (le plus souvent après une expérience préliminaire cependant), permet de déterminer le nombre d’observations nécessaires pour détecter un effet d’une taille donnée à un niveau fixe de probabilité (la puissance). Ce type d’analyse a priori devrait être réalisé plus souvent. Estimer la “limite de détection” statistique L’effort d’échantillonnage est souvent déterminé à l’avance (par exemple lorsque vous héritez de données récoltées par quelqu’un d’autre), ou très sévèrement limité (lorsque les contraintes logistiques prévalent). Que ce soit a priori ou a posteriori l’analyse de puissance vous permet d’estimer, pour un effort d’échantillonnage donné et un niveau de puissance fixe, quelle est la taille minimale de l’effet qui peut être détecté (comme étant statistiquement significatif). 2.1.3 Facteurs qui affectent la puissance Il y a 3 facteurs qui affectent la puissance d’un test statistique. Le critère de décision La puissance dépend de \\(\\alpha\\), le seuil de probabilité auquel on rejette l’hypothèse nulle. Si ce seuil est très strict (i.e. si \\(\\alpha\\) est fixé à une valeur très basse, comme 0.1% ou p = 0.001), alors la puissance sera plus faible que si le seuil était moins strict. La taille de l’échantillon Plus l’échantillon est grand, plus la puissance est élevée. La capacité d’un test à détecter de petites différences comme étant statistiquement significatives augmente avec une augmentation du nombre d’observations. La taille de l’effet Plus la taille de l’effet est grande, plus un test a de puissance. Pour un échantillon de taille fixe, la capacité d’un test à détecter un effet comme étant statistiquement significatif est plus élevée si l’effet est grand que s’il est petit. La taille de l’effet est en fait une mesure du degré de fausseté de l’hypothèse nulle. 2.2 Qu’est ce que G*Power? G*Power est un programme gratuit, développé par des psychologues de l’Université de Dusseldorf en Allemagne. Le programme existe en version Mac et Windows. Il peut cependant être utilisé sous linux via Wine. G*Power vous permettra d’effectuer une analyse de puissance pour la majorité des tests que nous verrons au cours de la session sans avoir à effectuer des calculs complexes ou farfouiller dans des tableaux ou des figures décrivant des distributions ou des courbes de puissance. Il est possible de faire tous les analyses de G*power avec R, mais cela est nettement plus complexes, car il faut tous coder à la main. Dans les cas les plus simple le code R est aussi fourni. G*power est vraiment un outil très utile que vous devrez maîtriser. Téléchargez le programme ici et installez-le sur votre ordi et votre station de travail au laboratoire (si ce n’est déjà fait).* 2.3 Comment utiliser G*Power 2.3.1 Principe général L’utilisation de G*Power implique généralement en trois étapes: Choisir le test approprié Choisir l’un des 5 types d’analyses de puissance disponibles Inscrire les valeurs des paramètres requis et cliquer sur Calculate 2.3.2 Types d’analyses de puissance disponibles A priori Calcule l’effectif requis pour une valeur de \\(\\alpha\\), \\(\\beta\\) et de taille d’effet donnée. Ce type d’analyse est utile à l’étape de planification des expériences. Compromis Calcule \\(\\alpha\\) et \\(\\beta\\) pour un rapport \\(\\beta / \\alpha\\) donné, un effectif fixe, et une taille d’effet donnée. Ce type d’analyse est plus rarement utilisé (je ne l’ai jamais fait), mais peut être utile lorsque le rapport \\(\\beta / \\alpha\\) est d’intérêt, par exemple lorsque le coût d’une erreur de type I et de type II peut être quantifié. Critère Calcule \\(\\alpha\\) pour \\(\\beta\\), effectif et taille d’effet donné. En pratique, je vois peu d’utilité pour ce type de calcul. Contactez-moi si vous en voyez une! Post-hoc Calcule la puissance (1 - \\(\\beta\\)) pour \\(\\alpha\\), une taille d’effet et un effectif donné. Très utilisée pour interpréter les résultats d’une analyse statistique non-significative, mais seulement si l’on utilise une taille d’effet biologiquement significative (et non la taille d’effet observée). Peu pertinente lorsque le test est significatif. Sensitivité Calcule la taille d’effet détectable pour une valeur d’\\(\\alpha\\), \\(\\beta\\) et un effectif donné. Très utile également au stade de planification des expériences. 2.3.3 Comment calculer la taille de l’effet G*Power permet de faire une analyse de puissance pour de nombreux tests statistiques L’indice de la taille de l’effet qui est utilisé par G*Power pour les calculs dépend du test. Notez que d’autres logiciels peuvent utiliser des indices différents et il est important de vérifier que l’indice que l’on utilise est celui qui convient. G*Power vous facilite la tâche et permet de calculer la taille de l’effet en inscrivant seulement les valeurs pertinentes dans la fenêtre de calcul. Le tableau suivant donne les indices utilisés par G*Power pour les différents tests. Test Taille d’effet Formule test de t sur des moyennes d \\(d = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{({s_1}^2 + {s_2}^2)/2}}\\) test de t pour des corrélations r autres tests de t f \\(f = \\frac{\\mu_1}{\\sigma}\\) test F (ANOVA) f \\(f = \\frac{\\frac{\\sqrt{\\sum_{i=1}^k (\\mu_i - \\mu)^2}}{k}}{\\sigma}\\) autres test F \\(f^2\\) \\(f^2 = \\frac{{R_p}^2}{1-{R_p}^2}\\) \\({R_p}\\) est le coefficient de corrélation partiel test Chi-carré w \\(w = \\sqrt{ \\sum_{i=1}^m \\frac{(p_{0i} - p_{1i})^2 }{ p_{0i}} }\\) \\(p_{0i}\\) \\(p_{1i}\\) sont les proportions de la catégorie i prédites par l’hypothèse nulle et alternative respectivement 2.4 Puissance pour un test de t comparant deux moyennes L’objectif de cette séance de laboratoire est de vous familiariser avec G*Power et de vous aider à comprendre comment les quatre paramètres des analyses de puissance (\\(\\alpha\\), \\(\\beta\\), effectif et taille de l’effet) sont reliés entre eux. On examinera seulement un des nombreux tests, le test de t permettant de comparer deux moyennes indépendantes. C’est le test le plus communément utilisé par les biologistes, vous l’avez tous déjà utilisé, et il conviendra très bien pour les besoins de la cause. Ce que vous apprendrez aujourd’hui s’appliquera à toutes les autres analyses de puissance que vous effectuerez à l’avenir. Jaynie Stephenson a étudié la productivité des ruisseaux de la région d’Ottawa. Elle a, entre autres, quantifié la biomasse des poissons dans 18 ruisseaux sur le Bouclier Canadien d’une part, et dans 18 autres ruisseaux de la vallée de la rivière des Outaouais et de la rivière Rideau d’autre part. Elle a observé une biomasse plus faible dans les ruisseaux de la vallée (2.64 \\(g/m^2\\) , écart-type=3.28) que dans ceux du Bouclier (3.31 \\(g/m^2\\) , écart-type=2.79). En faisant un test de t pour éprouver l’hypothèse nulle que la biomasse des poissons est la même dans les deux régions, elle obtient: Pooled-Variance Two-Sample t-Test t = -0.5746, df = 34, p-value = 0.5693 Elle accepte l’hypothèse nulle (puisque p est plus élevé que 0.05) conclue donc que la biomasse moyenne des poissons est la même dans ces deux régions. 2.4.1 Analyse post-hoc Compte tenu des valeurs des moyennes observées et de leur écart- type, on peut utiliser G*Power pour calculer la puissance du test de t bilatéral pour deux moyennes indépendantes et pour la taille d’effet (i.e. la différence entre la biomasse entre les deux régions, pondérée par les écarts-type) à \\(\\alpha\\) = 0.05. Démarrer G*Power. À Test family, choisir: t tests À Statistical test, choisir: Means: Difference between two inde- pendent means (two groups) À Type of power analysis, choisir: Post hoc: Compute achieved power - given \\(\\alpha\\), sample size, and effect size Dans Input Parameters, à la boîte Tail(s), choisir: Two, vérifier que \\(\\alpha\\) err prob est égal à 0.05 inscrire 18 pour Sample size group 1 et 2 pour calculer la taille d’effet (Effect size d), cliquer sur le bouton Determine =&gt; Dans la fenêtre qui s’ouvre à droite, sélectionner n1 = n2 entrer les moyennes (Mean group 1 et 2) entrer les écarts types (SDs group 1 et 2) cliquer sur le bouton Calculate and transfer to main window Cliquer sur le bouton Calculate dans la fenêtre principale et vous devriez obtenir ceci: Figure 2.1: Analyse post-hoc avec la taille d’effet estimée Étudions un peu ce graphique. La courbe de gauche, en rouge, correspond à la distribution de la statistique t si \\(H_0\\) est vraie (i.e si les deux moyennes étaient égales) compte tenu de l’effectif (18 dans chaque région) et des écarts- types observés. Les lignes verticales vertes correspondent aux valeurs critiques de t pour une valeur \\(\\alpha = 0.05\\) et un effectif total de 36 (2x18). Les régions ombrées en rose correspondent aux zones de rejet de \\(H_0\\). Si Jaynie avait obtenu une valeur de t en dehors de l’intervalle délimité par les valeurs critiques allant de -2.03224 à 2.03224, alors elle aurait rejeté \\(H_0\\) , l’hypothèse nulle d’égalité des deux moyennes. En fait, elle a obtenu une valeur de t égale à -0.5746 et conclu que la biomasse est la même dans les deux régions. La courbe de droite, en bleu, correspond à la distribution de la sta- tistique t si \\(H_1\\) est vraie (ici \\(H_1\\) correspond à une différence de biomasse entre les deux régions de \\(3.33-2.64=0.69g/m^2\\) , compte tenu des écarts-types observés). Cette distribution correspond à ce qu’on devrait s’attendre à observer si \\(H_1\\) était vraie et que l’on répétait un grand nombre de fois les mesures dans des échantillons aléatoires de 18 ruisseaux des deux régions en calculant la statistique t à chaque fois. En moyenne, on observerait une valeur de t d’environ 0.6. Notez que la distribution de droite chevauche considérablement celle de gauche, et une bonne partie de la surface sous la courbe de droite se retrouve à l’intérieur de l’intervalle d’acceptation de \\(H_0\\), délimité par les deux lignes vertes et allant de -2.03224 à 2.03224. Cette proportion, correspondant à la partie ombrée en bleu sous la courbe de droite et dénoté par \\(\\beta\\) correspond au risque d’erreur de type II qui est d’accepter \\(H_0\\) quand \\(H_1\\) est vraie. La puissance est simplement \\(1-\\beta\\), et est ici de 0.098339. Donc, si la biomasse différait de 0.69\\(g/m^2\\) entre les deux régions, Jaynie n’avait que 9.8% des chances d’être capable de détecter une différence statistiquement significative à \\(\\alpha=5%\\) en échantillonnant 18 ruisseaux de chaque région. Récapitulons: La différence de biomasse entre les deux régions n’est pas statistiquement significative d’après le test de t. C’est donc que cette différence est relativement petite compte tenu de la précision des mesures. Il n’est donc pas très surprenant que la puissance, i.e. la probabilité de détecter une différence significative, soit faible. Toute cette analyse ne nous informe pas beaucoup. Une analyse de puissance post hoc avec la taille de l’effet observé n’est pas très utile. On la fera plutôt pour une taille d’effet autre que celle observée quand H 0 est acceptée. Quelle taille d’effet utiliser? C’est la biologie du système étudié qui peut nous guider. Par exemple, en ce qui concerne la biomasse des poissons, on pourrait s’attendre à ce qu’une différence de biomasse du simple au double (disons de 2.64 à 5.28 \\(g/m^2\\) ) ait des conséquences écologiques. On voudrait s’assurer que Jaynie avait de bonnes chances de détecter une différence aussi grande que celle-là avant d’accepter ses conclusions que la biomasse est la même entre les deux régions. Quelles étaient les chances de Jaynie de détecter une différence de 2.64 \\(g/m^2\\) entre les deux régions? G*Power peut nous le dire. Changer la moyenne du groupe 2 à 5.28, recalculer la taille d’effet, et cliquer sur Calculate pour obtenir (2.2): Figure 2.2: Analyse post-hoc avec une taille d’effet différente La puissance est de 0.71, donc Jaynie avait une chance raisonnable de détecter une différence du simple au double avec 18 ruisseaux dans chaque région. Notez que cette analyse de puissance post hoc pour une taille d’effet jugée biologiquement significative est bien plus informative que l’analyse précédente pour la taille d’effet observée (qui est celle effectuée par défaut par bien des néophytes et de trop nombreux logiciels qui essaient de penser pour nous). En effet, Jaynie n’a pu détecter de différences significatives entre les deux régions. Cela pourrait être pour deux raisons: soit qu’il n’y a pas de différences entre les régions, ou soit parce que la précision des mesures est si faible et l’effort d’échantillonnage était si limité qu’il était très peu probable de détecter même d’énormes différences. La deuxième analyse de puissance permet d’éliminer cette seconde possibilité puisque Jaynie avait 71% des chances de détecter une différence du simple au double. 2.4.2 Analyse à priori Supposons qu’on puisse défendre la position qu’une différence de biomasse observée par Jaynie entre les deux régions, \\(3.31- 2.64=0.67g/m^2\\) , soit écologiquement signifiante. On devrait donc planifier la prochaine saison d’échantillonnage de manière à avoir de bonnes chances de détecter une différence de cette taille. Combien de ruisseaux Jaynie devrait-elle échantillonner pour avoir 80% des chances de la détecter (compte tenu de la variabilité observée)? Changer le type d’analyse de puissance dans G*Power à A priori: Compute sample size - given \\(\\alpha\\) , power, and effect size. Assurez-vous que les valeurs pour les moyennes et les écarts-type soient celles qu’a obtenu Jaynie, recalculez la taille de l’effet, et inscrivez 0.8 pour la puissance et vous obtiendrez: Figure 2.3: Analyse à priori Ouch! Il faudrait échantillonner 326 ruisseaux dans chaque région! Cela coûterait une fortune et exigerait de nombreuses équipes de travail. Sans cela, on ne pourrait échantillonner que quelques dizaines de ruisseaux, et il serait peu probable que l’on puisse détecter une si faible différence de biomasse entre les deux régions. Ce serait vraisemblablement en vain et pourrait être considéré comme une perte de temps: pourquoi tant d’efforts et de dépenses si les chances de succès sont si faibles. Si on refait le même calcul pour une puissance de 95%, on obtient 538 ruisseaux par région. Augmenter la puissance ça demande plus d’effort. 2.4.3 Analyse de sensitivité - Calculer la taille d’effet détectable Compte tenu de la variabilité observée, d’un effort d’échantillonnage de 18 ruisseaux par région, et en conservant \\(\\alpha=0.05\\), quelle est la taille d’effet que Jaynie pouvait détecter avec 80% de chances \\(\\beta=0.2\\))? Changez le type d’analyse dans G*Power à Sensitivity: Compute required effect size - given \\(\\alpha\\) , power, and sample size et assurez-vous que la taille des échantillons est de 18 dans chaque région. Vous obtiendrez: Figure 2.4: Analyse de sensitivité La taille d’effet détectable pour cette taille d’échantillon, \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (ou une puissance de 80%) est de 0.961296. Attention, cette valeur est l’indice d de la taille de l’effet et est pondérée par la variabilité des mesures. Dans ce cas ci, d est approximativement égal à \\[ d = \\frac{| \\bar{X_1} \\bar{X_2} |} {\\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}}\\] Pour convertir cette valeur de d sans unités en une valeur de différence de biomasse détectable (i.e \\(| \\bar{X_1} \\bar{X_2} |\\)), il suffit de multiplier d par le dénominateur de l’équation. \\[ | \\bar{X_1} \\bar{X_2} | = d * \\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}} \\] Donc, avec 18 ruisseaux dans chaque région, pour \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (une puissance de 80%), Jaynie pouvait détecter une différence de biomasse de 2.93\\(g/m^2\\) entre les régions, un peu plus que du simple au double. 2.5 Points à retenir L’analyse de puissance post hoc n’est pertinente que lorsque l’on a accepté l’hypothèse nulle. Il est en effet impossible de faire une erreur de type II quand on rejette \\(H_0\\). Avec de très grands échantillons, on a une puissance quasi infinie et on peut détecter statistiquement de très petites différences qui ne sont pas nécessairement biologiquement significatives. En utilisant un critère de signification plus strict (\\(\\alpha\\)&lt;0.05) on diminue notre puissance. En voulant maximiser la puissance, on augmente l’effort requis, à moins d’utiliser une valeur critique plus libérale (\\(\\alpha&gt;0.05\\)) Le choix de \\(\\beta\\) est quelque peu arbitraire. On considère que \\(\\beta=0.2\\) (puissance de 80%) est relativement élevé. 2.6 Exercice sur la puissance Les larves de mouches noires (Diptera: Simuliidae) ont été échantillonnées en février à l’émissaire de deux lacs des Cantons de l’Est (lacs Orford et Lovering). La longueur de chaque larve a été mesurée. Les données sont dans le fichier simulies.RData La relation entre la longueur (L, en mm) et la masse (M, en \\(\\mu\\)g) pour l’espèce dominante (P. mixtum/fuscum) est: \\[ M = 1.36 L^3.05 \\] Calculer la masse moyenne et l’écart-type à chaque site. En utilisant la masse moyenne de P. mixtum/fuscum à Lovering comme référence et les écarts-types observés aux 2 sites, calculer la puissance d’un test de t bilatéral pour moyennes indépendantes si la différence de masse est de 5 \\(\\mu\\)g, \\(\\alpha\\) = 0.05, et qu’on échantillonnait 100 larves à chaque site si la différence de masse est de 20 \\(\\mu\\)g, \\(\\alpha\\) = 0.05, et qu’on échantillonnait 100 larves à chaque site si la différence de masse est de 50\\(\\mu\\)g, \\(\\alpha\\) = 0.05, et qu’on échantillonnait 100 larves à chaque site Comment est-ce que la puissance varie avec la taille de l’effet? Calculer la taille d’échantillon requis pour pouvoir détecter, par un test de t bilatéral pour moyennes indépendantes en tenant compte des écarts-types observés, une différence de 50 \\(\\mu\\)g entre les moyennes avec une puissance de 80% et \\(\\alpha\\) = 0.05 avec une puissance de 80% et \\(\\alpha\\) = 0.001 avec une puissance de 95% et \\(\\alpha\\) = 0.05 Comment est-ce que la taille d’échantillon requise varie avec \\(\\alpha\\) et \\(\\beta\\)? Calculer la taille d’effet détectable (d) par un test de t bilatéral pour moyennes indépendantes, compte tenu des écarts-types observés avec une puissance de 80%, \\(\\alpha\\) = 0.05, et des mesures sur 10 larves de chaque site avec une puissance de 80%, \\(\\alpha\\) = 0.05, et des mesures sur 200 larves à chaque site avec une puissance de 80%, \\(\\alpha\\) =0.05, et des mesures sur 20 larves d’un site et sur 380 larves au second site Comment est-ce que la taille d’effet détectable dépend de la taille d’échantillon dans les 2 groupes? Calculer la différence de masse, en \\(\\mu\\)g, qui est détectable d’après vos estimés de la taille minimale d’effet détectable à 4a, b, et c. "],
["corrélation-et-régression-linéaire-simple.html", "3 Corrélation et régression linéaire simple 3.1 Paquets et données requises pour le labo 3.2 Diagrammes de dispersion 3.3 Transformations et le coefficient de corrélation 3.4 Matrices de corrélations et correction de Bonferroni 3.5 Corrélations non paramétriques: r de Spearman et tau de Kendall 3.6 Régression linéaire simple 3.7 Transformation des données en régression 3.8 Traitement des valeurs extrèmes 3.9 Quantifier la taille d’effet et analyse de puissance en régression 3.10 Bootstrap en régression simple avec R", " 3 Corrélation et régression linéaire simple Après avoir complété cet exercice de laboratoire, vous devriez être en mesure de : Utiliser R pour produire un diagramme de dispersion pour illus- trer la relation entre deux variables avec trace lowess Utiliser R pour faire des transformations simples Utiliser R pour calculer le coefficient de corrélation de Pearson entre deux variables et en évaluer sa signification statistique Utiliser R pour calculer la corrélation de rang entre des paires de variables avec le r de Spearman et le tau de Kendall Utiliser R pour évaluer la signification de corrélations dans une matrice de corrélation en utilisant les probabilités ajustées par la méthode de Bonferroni. Utiliser R pour faire une régression linéaire simple. Utiliser R pour évaluer si un ensemble de données remplit les conditions d’application d’une analyse de régression simple. Quantifier la taille de l’effet d’une régression simple et effectuer une analyse de puissance avec G*Power. 3.1 Paquets et données requises pour le labo Ce laboratoire nécessite: les paquets R: car lmtest boot les fichiers de données sturgeon.csv 3.2 Diagrammes de dispersion Les analyses de corrélation et de régression devraient toujours commencer par un examen des données.C’est une étape critique qui sert à évaluer si ce type d’analyse est approprié pour un ensemble de données. Supposons que nous sommes intéressés à évaluer si la longueur d’esturgeons mâles dans la région de The Pas covarie avec leur poids. Pour répondre à cette question, regardons d’abord la corrélation entre la longueur et le poids. Souvenez-vous qu’une des conditions d’application de l’analyse de corrélation est que la relation entre les deux variables est linéaire. Pour évaluer cela, commençons par faire un diagramme de dispersion. Les données sur les esturgeons son disponibles dans le fichier sturgeon.csv. Après avoir chargé les données dnas un objet sturgeon, faites un diagramme de dispersion avec une droite de régression et une courbe LOWESS de la longueur en fonction du poids. sturgeon &lt;- read.csv(&quot;data/sturgeon.csv&quot;) str(sturgeon) ## &#39;data.frame&#39;: 186 obs. of 9 variables: ## $ fklngth : num 37 50.2 28.9 50.2 45.6 ... ## $ totlngth: num 40.7 54.1 31.3 53.1 49.5 ... ## $ drlngth : num 23.6 31.5 17.3 32.3 32.1 ... ## $ rdwght : num 15.95 NA 6.49 NA 29.92 ... ## $ age : int 11 24 7 23 20 23 20 7 23 19 ... ## $ girth : num 40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ... ## $ sex : chr &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; ... ## $ location: chr &quot;THE_PAS&quot; &quot;THE_PAS&quot; &quot;THE_PAS&quot; &quot;THE_PAS&quot; ... ## $ year : int 1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ... mygraph&lt;-ggplot( data = sturgeon[!is.na(sturgeon$rdwght),], # source of data aes(x = fklngth, y = rdwght)) # plot data points, regression, loess trace mygraph &lt;- mygraph + stat_smooth(method=lm, se=FALSE, color=&quot;green&quot;) + # add linear regression, but no SE shading stat_smooth(color=&quot;red&quot;, se=FALSE) + #add loess geom_point() # add data points mygraph # display graph Figure 3.1: Graphique du poids en fonction de la longueur des esturgeons Est-ce que la dispersion des points suggère une bonne corrélation entre les deux variables? Est-ce que la relation semble linéaire? Ce graphique suggère une tendance plus curvilinéaire que linéaire. Malgré tout, il semble y avoir une forte corrélation entre les deux variables. Refaites le diagramme de dispersion avec des axes logarithmiques. # apply log transformation on defined graph mygraph + scale_x_log10() + scale_y_log10() Figure 3.2: Graphique poids-longueur avec une échelle log Comparez les diagrammes de dispersion avant et après transformation (Figures 3.1 et 3.2). Comme l’analyse de corrélation présuppose une relation linéaire entre les variables, on devrait donc privilégier l’analyse sur les données log-transformées. 3.3 Transformations et le coefficient de corrélation Une autre condition préalable à l’analyse de corrélation est que les deux variables concernées suivent une distribution normale bidimensionnelle. On peut aisément vérifier la normalité de chacune des 2 variables séparément tel que décrit dans le laboratoire précédent. Si les deux variables sont normalement distribuées, on présume généralement qu’elles suivent une distribution normale bidimensionnelle lorsqu’analysées simultanément (notez que ce n’est pas toujours le cas cependant). Examinez la distribution des quatre variables (les deux variables originales et les variables transformées). Que concluez-vous de l’inspection visuelle de ces graphiques ? Les figures ci-dessous sont les diagrammes de probabilité (qqplot()). Le code pour produire des graphiques multiples sur une page, comme on voit ci-dessous, est: par(mfrow = c(2, 2)) # divise le graphique en 4 sections qqnorm(sturgeon$fklngth,ylab=&quot;fklngth&quot;) qqline(sturgeon$fklngth) qqnorm(log10(sturgeon$fklngth),ylab=&quot;log10(fklngth)&quot;) qqline(log10(sturgeon$fklngth)) qqnorm(sturgeon$rdwght,ylab=&quot;rdwght&quot;) qqline(sturgeon$rdwght) qqnorm(log10(sturgeon$rdwght),ylab=&quot;log10(rdwgth)&quot;) qqline(log10(sturgeon$rdwght)) par(mfrow = c(1, 1)) #redéfinie la zone de graphique par défaut Il n’y a pas grand-chose à redire: aucune des distributions n’est parfaitement normale, mais les déviations semblent mineures. Générez une matrice de graphiques de dispersion améliorés en utilisant la commande scatterplotMatrix de la librairie car. scatterplotMatrix( ~fklngth+log10(fklngth)+rdwght+log10(rdwght), data= sturgeon, smooth=TRUE, diagonal = &#39;density&#39;) ## Warning in applyDefaults(diagonal, defaults = list(method = ## &quot;adaptiveDensity&quot;), : unnamed diag arguments, will be ignored Ensuite, calculez le coefficient de corrélation de Pearson entre chaque paire (variables originales et logtransformées ) en utilisant la commande cor(). Avant de commencer, on va cependant ajouter les variables transformées au tableau de données sturgeon: sturgeon$lfklngth &lt;- with(sturgeon, log10(fklngth)) sturgeon$lrdwght &lt;- log10(sturgeon$rdwght) Vous pouvez ensuite obtenir la matrice de corrélation par: cor(sturgeon[,c(&quot;fklngth&quot;,&quot;lfklngth&quot;,&quot;lrdwght&quot;,&quot;rdwght&quot;)], use=&quot;complete.obs&quot;) Fréquemment, il y a des données manquantes dans un échantillon. En choisissant use=\"complete.obs\", toutes les lignes du fichier pour lesquelles les variables ne sont pas toutes mesurées sont éliminées. Dans ce cas, toutes les corrélations seront calculées avec le même nombre de cas. Par contre, en utilisant use=\"pairwise.complete.obs\" , R élimine une observation que lorsqu’un des deux membres de la paire a une valeur manquante. Dans ce cas, si les données manquantes pour différentes variables se retrouvent dans un groupe différent d’observation, les corrélations ne seront pas nécessairement calculées sur le même nombre de cas ni sur le même sous-ensemble de cas. En général, vous devriez utiliser l’option use=\"complete.obs\" à moins que vous ayez un très grand nombre de données manquantes et que cette façon de procéder élimine la plus grande partie de vos observations. Pourquoi la corrélation entre les variables originales est-elle la plus faible des trois ? cor(sturgeon[,c(&quot;fklngth&quot;,&quot;lfklngth&quot;,&quot;lrdwght&quot;,&quot;rdwght&quot;)], use=&quot;complete.obs&quot;) ## fklngth lfklngth lrdwght rdwght ## fklngth 1.0000000 0.9921435 0.9645108 0.9175435 ## lfklngth 0.9921435 1.0000000 0.9670139 0.8756203 ## lrdwght 0.9645108 0.9670139 1.0000000 0.9265513 ## rdwght 0.9175435 0.8756203 0.9265513 1.0000000 Il y a plusieurs choses à noter ici. Premièrement, la corrélation entre la longueur à la fourche et le poids rond est élevée, peu importe la transformation: les poissons lourds ont tendance à être longs. Deuxièmement, la corrélation est plus forte pour les données transformées que pour les données brutes. Pourquoi? Parce que le coefficient de corrélation est inversement proportionnel au bruit autour de la relation linéaire. Si la relation est curvilinéaire (comme dans le cas des données non transformées), le bruit est plus grand que si la relation est parfaitement linéaire. Par conséquent, la corrélation est plus faible. 3.4 Matrices de corrélations et correction de Bonferroni Une pratique courante est d’examiner une matrice de corrélation à la recherche des associations significatives. Comme exemple, essayons de tester si la corrélation entre lfklngth et rdwght est significative (c’est le plus faible coefficient de corrélation de cette matrice). Estimer la correlation entre la longueur (fklngth) et le poids (rdwght) des esturgeons: cor.test( sturgeon$lfklngth, sturgeon$rdwght, alternative=&quot;two.sided&quot;, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: sturgeon$lfklngth and sturgeon$rdwght ## t = 24.322, df = 180, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8367345 0.9057199 ## sample estimates: ## cor ## 0.8756203 On voit ici que la corrélation est hautement significative (p&lt; 2.2e-16),ce qui n’est pas surprenant étant donné la valeur du coefficient de corrélation (0.8756). Il est important de réaliser que si une matrice contient un grand nombre de corrélations, il n’est pas surprenant d’en trouver au moins une qui soit “significative”. En effet, on s’attend à en trouver 5% en moyenne lorsqu’il n’y a en fait aucune corrélation entre les paires de moyennes. Une façon de corriger pour cette tendance est d’ajuster le niveau \\(\\alpha\\) critique auquel on attribue une signification statistique en divisant \\(\\alpha\\) par le nombre \\(k\\) de corrélations qui sont examinées : \\(\\alpha&#39; = \\alpha / k\\) (ajustement de Bonferroni). Si initialement \\(\\alpha = 0.05\\) et qu’il y a 10 corrélations qui sont examinées, alors \\(\\alpha&#39;= 0.005\\). Donc, afin de rejeter l’hypothèse nulle, la valeur de p devra être plus petite que \\(\\alpha&#39;\\), en l’occurrence 0.005. Dans l’exemple qui précède, on devrait donc ajuster \\(\\alpha\\) critique en divisant par le nombre total de corrélations dans la matrice (6 dans ce cas, donc \\(\\alpha&#39;=0.00833\\)). Cette correction modifie-t-elle votre conclusion quant à la corrélation entre lkfl et rdwght? 3.5 Corrélations non paramétriques: r de Spearman et tau de Kendall L’analyse faite à la section précédente avec les esturgeons suggère que l’une des conditions préalables à l’analyse de corrélation, soit la distribution normale bidimensionnelle de données, pourrait ne pas être remplie pour fklngth et rdwght, ni pour les paires de variables transformées. La recherche d’une transformation appropriée peut parfois être difficile. Pire encore, pour certaines distributions il n’existe pas de transformation qui va normaliser les données. Dans ces cas-là, la meilleure option est de faire une analyse non paramétrique qui ne présume ni de la normalité ni de la linéarité. Ces analyses sont basées sur les rangs. Les deux plus communes sont le coefficient de rang de Spearman et le tau de Kendall. Dans R, testez la corrélation entre fklngth et rdwght en utilisant Spearman et Kendall’s . cor.test( sturgeon$lfklngth, sturgeon$rdwght, alternative=&quot;two.sided&quot;, method=&quot;spearman&quot;) ## Warning in cor.test.default(sturgeon$lfklngth, sturgeon$rdwght, alternative = ## &quot;two.sided&quot;, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: sturgeon$lfklngth and sturgeon$rdwght ## S = 47971, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9522546 cor.test( sturgeon$lfklngth, sturgeon$rdwght, alternative=&quot;two.sided&quot;, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: sturgeon$lfklngth and sturgeon$rdwght ## t = 24.322, df = 180, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8367345 0.9057199 ## sample estimates: ## cor ## 0.8756203 Comparer les résultats de cette analyse à l’analyse paramétrique. Pourquoi y-a-t’il une différence ? Calculez les corrélations non paramétriques sur les paires de variables transformées. Vous devriez voir tout de suite que les corrélations des données transformées et non transformées sont identiques puisque dans les deux cas la corrélation est calculée à partir des rangs qui ne sont pas affectés par la transformation. Notez que les corrélations obtenues avec le tau de Kendall (0.820) sont plus faibles que celles du coefficient de Spearman (0.952). Le tau de Kendall pondère un peu plus les grandes différences entre les rangs alors que le coefficient de Spearman donne le même poids à chaque paire d’observations. En général, on préfère le tau de Kendall lorsqu’il y a plus d’incertitude quant aux rangs qui sont près les uns des autres. Les esturgeons de cet échantillon ont été capturés à l’aide de filets et d’hameçons d’une taille fixe. Quel impact cette méthode de capture peut-elle avoir eu sur la forme de la distribution de fklngth et rdwght? Compte tenu de ces circonstances, l’analyse de corrélation est-elle appropriée ? Rappelez-vous que l’analyse de corrélation présume aussi que chaque variable est échantillonnée aléatoirement. Dans le cas de nos esturgeons, ce n’est pas le cas: les hameçons appâtés et les filets ne capturent pas de petits esturgeons (et c’est pourquoi il n’y en a pas dans l’échantillon). Il faut donc réaliser que les coefficients de corrélation obtenus dans cette analyse ne reflètent pas nécessairement ceux de la population totale des esturgeons. 3.6 Régression linéaire simple L’analyse de corrélation vise à décrire comment deux variables covarient. L’analyse de régression vise plutôt à produire un modèle permettant de prédire une variable (la variable dépendante) par l’autre (la variable indépendante). Comme pour l’analyse de corrélation, on devrait commencer en examinant des graphiques. Puisque l’on est intéressé à quantifier la relation entre deux variables, un graphique de la variable dépendante (Y) en fonction de la variable indépendante (X) est tout à fait approprié. Le fichier sturgeon.csv contient des données d’un inventaire des esturgeons récoltés en 1978-1980 à Cumberland House en Saskatchewan et à The Pas au Manitoba. Faites un diagramme de dispersion de fklngth (la variable dépendante) en fonction de age (la variable indépendante) pour les esturgeons mâles unqiuement et ajoutez-y une régression linéaire et une trace lowess. Que concluez-vous de ce diagramme de dispersion ? sturgeon.male &lt;- subset(sturgeon, subset = sex == &quot;MALE&quot;) mygraph &lt;- ggplot( data=sturgeon.male, # source of data aes(x=age, y=fklngth)) #aesthetics: y=fklngth, x=rdwght # plot data points, regression, loess trace mygraph &lt;- mygraph + stat_smooth(method=lm, se=FALSE, color=&quot;green&quot;) + # add linear regression, but no SE shading stat_smooth(color=&quot;red&quot;) + #add loess geom_point() # add data points mygraph # display graph Ce graphique suggère que la relation n’est pas linéaire. Supposons que nous désirions estimer le taux de croissance des esturgeons mâles. Un estimé (peut-être pas terrible…) du taux de croissance peut être obtenu en calculant la pente de la régression de la longueur à la fourche sur l’âge. Ajustons d’abord une régression avec la commande lm() et sauvons ces résultats dans un objet appelé RegModel.1. RegModel.1 &lt;- lm(fklngth ~ age, data = sturgeon.male) Rien n’apparait à l’écran, c’est normal ne vous inquiétez pas, tout a été sauvegardé en mémoire. Pour voir les résultats, tapez: summary(RegModel.1) ## ## Call: ## lm(formula = fklngth ~ age, data = sturgeon.male) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.4936 -2.2263 0.1849 1.7526 10.8234 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.50359 1.16873 24.39 &lt;2e-16 *** ## age 0.70724 0.05888 12.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.307 on 73 degrees of freedom ## (5 observations deleted due to missingness) ## Multiple R-squared: 0.664, Adjusted R-squared: 0.6594 ## F-statistic: 144.3 on 1 and 73 DF, p-value: &lt; 2.2e-16 Le modèle qui a été ajusté et les données utilisées. Un sommaire statistique des résidus autour du modèle estimé. Valeurs estimées des paramètres du modèle, erreurs-types, statistiques t et probabilités associées. Racine carrée de la variance résiduelle. Coefficient de détermination. Il correspond à la proportion de la variabilité de la variable dépendante qui peut être expliquée par la régression. Le R-carré ajusté tient compte du nombre de paramètres du modèle. Si vous voulez comparer différents modèles qui n’ont pas le même nombre de paramètres, c’est ce qu’il faut utiliser. C’est le test de signification omnibus du modèle. Dans le cas de la régression simple, il est équivalent au test sur la pente de la régression. La régression estimée est donc: \\[ Fklngth = 28.50359 + 0.70724 * age\\] Étant donné la valeur significative du test de F ainsi que pour le test de t pour la pente de la droite, on rejette l’hypothèse nulle qu’il n’y a pas de relation entre la taille et l’âge. 3.6.1 Vérifier les conditions d’application de la régression La régression simple de type I a quatre conditions préalables : il n’y a pas d’erreur de mesure sur la variable indépendante (X) la relation entre Y et X est linéaire les résidus sont normalement distribués la variance des résidus est constante pour toutes les valeurs de la variable indépendante Procédons maintenant à l’examen post-mortem. La première condition est rarement remplie avec des données biologiques ; il y presque toujours de l’erreur sur X et sur Y. Cela veut dire qu’en général les pentes estimées sont biaisées, mais que les valeurs prédites ne le sont pas. Toutefois, si l’erreur de mesure sur X est petite par rapport à l’étendue des valeurs de X, le résultat de l’analyse n’est pas dramatiquement influencé. Par contre, si l’erreur de mesure est relativement grande (toujours par rapport à l’étendue des valeurs de X), la droite de régression obtenue par la régression de modèle I est un piètre estimé de la relation fonctionnelle entre X et Y. Dans ce cas, il est préférable de passer à la régression de modèle II, malheureusement au-delà du contenu de ce cours. Les autres conditions préalables à l’analyse de régression de modèle I peuvent cependant être vérifiées, ou du moins évaluées visuellement. La commande plot() permet de visualiser des graphiques diagnostiques pour des modèles linéaires. par(mfrow = c(2, 2), las=1) plot(RegModel.1) La commande par() est utilisée pour dire à R de tracer 2 rangées et 2 colonnes de graphiques par page (il y a quatre graphiques diagnostiques qui sont générés automatiquement pour les modèles linéaires), et la commande las indique à R d’effectuer une rotation des légendes de l’axe des Y pour qu’elles soient perpendiculaires à l’axe (oui. Je sais. Rien de tout ça n’est évident.) Vous obtiendrez: En haut à gauche, permet d’évaluer la linéarité, la normalité, et l’homoscédasticité des résidus. Il illustre les déviations autour de la régression en fonction des valeurs prédites. Rappllez-vous que le graphique de fklngth vs age suggère que la relation entre la longueur à la fourche et l’âge n’est pas linéaire. Les très jeunes et très vieux esturgeons sont sous la droite en général, alors que les esturgeons d’âge moyen sont retrouvés généralement au-dessus de la droite de régression. C’est exactement ce que le graphique des résidus en fonction des valeurs prédites illustre. La ligne en rouge est une trace lowess au travers de ce nuage de points. Si la relation était linéaire, la trace lowess serait presque plate et près de 0. La dispersion des résidus permet d’évaluer visuellement leur normalité et hétéroscédasticité; mais ce graphique n’est pas optimal pour évaluer ces propriétés. Les deux graphiques suivants sont supérieurs au premier pour cela. En haut à droite permet d’évaluer la normalité des résidus. C’est un graphique QQ des résidus (QQ plot). Des résidus distribués normalement tomberaient exactement sur la diagonale. Ici, on voit que c’est presque le cas, sauf dans les queues de la distribution. En bas à gauche, intitulé Scale-Location, permet d’évaluer l’homoscedasticité. On y retrouve sur l’ordonnée (l’axe des y) la racine carrée de la valeur absolue des résidus standardisés (résidus divisés par l’écart-type des résidus) en fonction des valeurs prédites. Le graphique aide à déterminer si la variation des résidus est constante ou non. Si les résidus sont homoscédastiques, la valeur moyenne sur l’axe des y ne va pas changer en fonction de la valeur prédite. Ici, il y a une certaine tendance, mais pas une tendance monotone puisqu’ il y a d’abord une baisse puis une hausse..; bref, rien qui soit une forte évidence contre la supposition d’homoscédasticité. En bas à droite, montre les résidus en fonction du “leverage” et permet de détecter certaines valeurs extrêmes qui ont une grande influence sur la régression. Le leverage d’un point mesure sa distance des autres points, mais seulement en ce qui concerne les variables indépendantes. Dans le cas d’une régression simple, cela revient à la distance entre le point sur l’axe des x et la moyenne de tous les points sur cet axe. Vous devriez porter une attention particulière aux observations qui ont un leverage plus grand que \\(2(k+1)/n\\), où k est le nombre de variables indépendantes (ici, 1) et n est le nombre d’observations. Dans cet exemple, il y a 75 observations et une variable indépendante et donc les points ayant un leverage plus grand que \\(4 / 75 = 0.053\\) devrait être considérés avec attention. Le graphique indique également comment la régression changerait si on enlevait un point. Ce changement est mesuré par la distance de Cook, illustrée par les bandes en rouge sur le graphique. Un point ayant une distance de Cook supérieure à 1 a une grande influence. Notez que R identifie automatiquement les cas les plus extrèmes sur chacun de ces 4 graphiques. Le fait qu’un point soit identifié ne signifie pas nécessairement que c’est une valeur réellement extrème, ou que vous devez vous en préoccuper. Dans tous les ensembles de données il y aura toujours un résidu plus grand que les autres… Finalement, quel est le verdict concernant la régression linéaire entre fklngth et age ? Elle viole la condition de linéarité, possiblement celle de normalité, remplit la condition d’homoscédasticité, et ne semble pas influencée outre mesure par des valeurs bizarres ou extrêmes. 3.6.2 Tests formels des conditions d’application pour la régression Personnellement, je n’utilise jamais les tests formels des conditions d’application de la régression et me contente des graphiques des résidus pour guider mes décisions. C’est ce que la plupart des praticiens font. Cependant, lors de mes premières analyses, je n’étais pas toujours certain de bien interpréter les graphiques et j’aurais aimé un indice plus formel ou un test permettant de détecter les violations des conditions d’application de la régression. Le package lmtest , qui ne fait pas partie de l’installation de base, mais qui est disponible sur CRAN, permet de faire plusieurs tests de linéarité et d’homoscédasticité. Et on peut tester la normalité avec le test Shapiro-Wilk test vu précédemment. Charger le package lmtest de CRAN (et installer le si besoin). library(lmtest) Exécutez les commandes suivantes bptest(RegModel.1) ## ## studentized Breusch-Pagan test ## ## data: RegModel.1 ## BP = 1.1765, df = 1, p-value = 0.2781 Le test Breusch-Pagan test examine si la variabilité des résidus est constantes lorsque les valeurs prédites changent. Une faible valeur de p suggère de l’hétéroscédasticité. Ici, la valeur p est élevée et suggère que la condition d’application d’homoscédasticité est remplie avec ces données. dwtest(RegModel.1) ## ## Durbin-Watson test ## ## data: RegModel.1 ## DW = 2.242, p-value = 0.8489 ## alternative hypothesis: true autocorrelation is greater than 0 Le test Durbin-Watson permet de détecter l’autocorrélation sérielle des résidus. En l’absence d’autocorrélation (i.e. d’indépendance des résidus) la valeur attendue de la statistique D est 2. Ce test permet d’éprouver l’hypothèse d’indépendance des résidus, mais ne permet de détecter qu’un type particulier de dépendance. Ici, le test ne permet pas de rejeter l’hypothèse d’indépendance. resettest(RegModel.1) ## ## RESET test ## ## data: RegModel.1 ## RESET = 14.544, df1 = 2, df2 = 71, p-value = 5.082e-06 Le test RESET permet d’éprouver la linéarité. Si la relation est linéaire, alors la statistique RESET sera d’environ 1. Ici, la statistique est beaucoup plus élevée (14.54) et hautement significative. Le test confirme la tendance que nous avons détectée visuellement plus haut: la relation n’est pas linéaire. shapiro.test(residuals(RegModel.1)) ## ## Shapiro-Wilk normality test ## ## data: residuals(RegModel.1) ## W = 0.98037, p-value = 0.2961 Le test de normalité Shapiro-Wilk sur les résidus confirme que la déviation par rapport à une distribution normale des résidus n’est pas grande. 3.7 Transformation des données en régression La relation entre fklngth et age n’étant pas linéaire, on devrait donc essayer de transformer les données pour tenter de les linéariser : Voyons ce qu’une transformation log donne: par(mfrow = c(1, 1), las=1) ggplot( data = sturgeon.male, aes(x=log10(age), y=log10(fklngth))) + geom_smooth(color=&quot;red&quot;)+ geom_smooth(method=&quot;lm&quot;, se=FALSE, color=&quot;green&quot;)+ geom_point() Ajustons maintenant une régression simple sur ces données transformées. RegModel.2 &lt;- lm(log10(fklngth)~log10(age), data=sturgeon.male) summary(RegModel.2) ## ## Call: ## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.082794 -0.016837 -0.000719 0.021102 0.087446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.19199 0.02723 43.77 &lt;2e-16 *** ## log10(age) 0.34086 0.02168 15.72 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03015 on 73 degrees of freedom ## (5 observations deleted due to missingness) ## Multiple R-squared: 0.772, Adjusted R-squared: 0.7688 ## F-statistic: 247.1 on 1 and 73 DF, p-value: &lt; 2.2e-16 Examinons maintenant les graphiques diagnostiques: par(mfrow = c(2, 2), las=1) plot(RegModel.2) Il y a une certaine amélioration, mais ce n’est pas encore parfait (la perfection n’est pas de ce monde….). Le graphique des résidus en fonction des valeurs prédites suggère encore une certaine non linéarité. Sur le graphique Q-Q les points se retrouvent plus près de la droite diagonale qu’avant, indiquant que les résidus sont encore plus près de la normalité après la transformation log-log. Il n’y a pas d’indice d’hétéroscédasticité. Finalement, même si il reste quelques points avec plus d’influence (leverage) que les autres, aucun n’a une distance de Cook au-delà de 0.5. En résumé, la transformation log a amélioré les choses: relation est plus linéaire, les résidus sont plus normaux, et il y a moins de points avec une influence relativement élevée.Est-ce que les tests formels supportent cette évaluation? bptest(RegModel.2) ## ## studentized Breusch-Pagan test ## ## data: RegModel.2 ## BP = 0.14282, df = 1, p-value = 0.7055 dwtest(RegModel.2) ## ## Durbin-Watson test ## ## data: RegModel.2 ## DW = 2.1777, p-value = 0.6134 ## alternative hypothesis: true autocorrelation is greater than 0 resettest(RegModel.2) ## ## RESET test ## ## data: RegModel.2 ## RESET = 4.4413, df1 = 2, df2 = 71, p-value = 0.01523 shapiro.test(residuals(RegModel.2)) ## ## Shapiro-Wilk normality test ## ## data: residuals(RegModel.2) ## W = 0.98998, p-value = 0.8246 Oui, les conclusions sont les mêmes: les résidus sont encore homoscédastiques (test Breusch-Pagan), ne sont pas autocorrélés (test Durbin-Watson), sont normaux (test Shapiro-Wilk ), et sont plus linéaires (la valeur de P du test RESET est maintenant 0.015, au lieu de 0.000005). Donc la linéarité a augmenté, mais cette condition d’application semble encore légèrement violée. 3.8 Traitement des valeurs extrèmes Dans cet exemple, il n’y a pas de valeur vraiment extrème. Oui, je sais, R a quand même identifié les observations 8, 24, et 112 dans le dernier graphique diagnostique. Mais ces valeurs sont encore dans la fourchette de valeurs que je juge “acceptables”. Mais comment déterminer objectivement ce qui est acceptable? À quel moment juge t’on qu’une valeur extrême est vraiment trop invraisemblable pour ne pas l’exclure? Il n’y a malheureusement pas de règle absolue là-dessus. Les opinions varient, mais je penche vers le conservatisme sur cette question. Ma position est que, à moins que la valeur soit biologiquement impossible ou clairement une erreur d’entrée de données, je n’élimine pas les valeurs extrêmes et j’utilise toutes mes données dans leur analyse. Pourquoi? Parce que je veux que mes données reflètent bien la variabilité naturelle ou réelle. C’est d’ailleurs parfois cette variabilité qui est intéressante. L’approche conservatrice qui consiste à conserver toutes les valeurs extrêmes possibles est possiblement la plus honnête, mais elle peut causer certains problèmes. Ces valeurs extrêmes sont souvent la cause des violations des conditions d’application des tests statistiques. La solution suggérée à ce dilemme est de faire l’analyse avec et sans les valeurs extrêmes et de comparer les conclusions. Dans bien des cas, les conclusions seront qualitativement les mêmes et les tailles d’effet ne seront pas très différentes. Toutefois, dans certains cas, la présence des valeurs extrêmes change complètement les conclusions. Dans ces cas, il faut simplement accepter que les conclusions dépendent entièrement de la présence des valeurs extrêmes et sont donc peu concluantes. Suivant cette approche comparative, refaisons donc l’analyse après avoir enlevé les observations 8, 24, et 112. RegModel.3 &lt;- lm(log10(fklngth)~log10(age), data=sturgeon.male, subset = !(rownames(sturgeon.male) %in% c(&#39;8&#39;,&#39;24&#39;,&#39;112&#39;))) summary(RegModel.3) ## ## Call: ## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male, ## subset = !(rownames(sturgeon.male) %in% c(&quot;8&quot;, &quot;24&quot;, &quot;112&quot;))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.069163 -0.017390 0.000986 0.018590 0.047647 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.22676 0.02431 50.46 &lt;2e-16 *** ## log10(age) 0.31219 0.01932 16.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02554 on 70 degrees of freedom ## (5 observations deleted due to missingness) ## Multiple R-squared: 0.7885, Adjusted R-squared: 0.7855 ## F-statistic: 261 on 1 and 70 DF, p-value: &lt; 2.2e-16 L’ordonnée à l’origine (Intercept), la pente, et le R carré sont presque les mêmes, et la valeur de p est encore astronomiquement petite. Enlever les valeurs extrêmes a peu d’effet dans ce cas. Les graphiques diagnostiques des résidus et les tests formels des conditions d’application sur ce sous-ensemble de données donnent: par(mfrow = c(2,2)) plot(RegModel.3) sturgeon.male.subset &lt;- subset(sturgeon, subset=!(rownames(sturgeon.male) %in% c(&#39;8&#39;,&#39;24&#39;,&#39;112&#39;))) bptest(RegModel.3) ## ## studentized Breusch-Pagan test ## ## data: RegModel.3 ## BP = 0.3001, df = 1, p-value = 0.5838 dwtest(RegModel.3) ## ## Durbin-Watson test ## ## data: RegModel.3 ## DW = 2.0171, p-value = 0.5074 ## alternative hypothesis: true autocorrelation is greater than 0 resettest(RegModel.3) ## ## RESET test ## ## data: RegModel.3 ## RESET = 3.407, df1 = 2, df2 = 68, p-value = 0.0389 shapiro.test(residuals(RegModel.3)) ## ## Shapiro-Wilk normality test ## ## data: residuals(RegModel.3) ## W = 0.98318, p-value = 0.4502 Il n’y a pas vraiment de différence ici non plus avec l’analyse des données en entier. Bref, tout pointe vers la conclusion que les valeurs les plus extrêmes de cet ensemble de donnée n’influencent pas indûment les résultats statistiques. 3.9 Quantifier la taille d’effet et analyse de puissance en régression L’interprétation biologique des résultats n’est pas la même chose que l’interprétation statistique. Dans l’analyse qui précède, on conclue statistiquement que la taille augmente avec l’âge (puisque la pente est positive et et p&lt;0.05). Mais cette augmentation “statistique” de la taille avec l’âge ne donne pas d’information sur la différence de taille entre les jeunes et vieux individus. La pente et un graphique sont plus informatifs à ce sujet que la valeur p. La pente (dans l’espace log-log) est 0.34. Cela veut dire que pour chaque unité d’accroissement de X (log10(age)), il y a une augmentation de 0.34 unités de log10(fklngth). En d’autres mots, quand l’âge est multiplié par 10, la longueur à la fourche est multipliée environ par 2. Donc la longueur des esturgeons augmente plus lentement que leur âge (contrairement à mon tour de taille, semble-t-il….). La valeur de la pente (0.34) est un estimé de la taille de l’effet de l’âge sur la longueur. 3.9.1 Puissance de détecter une pente donnée Pour les calculs de puissance avec G*Power vous devrez cependant utiliser une autre métrique de la taille de l’effet, calculée à partir de la pente, de son erreur-type, et de la taille de l’échantillon (ce qui facilite les calculs pour G*Power, mais malheureusement pas pour vous ;-) La métrique (d) est calculée comme: \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} \\] où \\(b\\) est l’estimé de la pente, \\(s_b\\) est l’erreur type de la pente, \\(n\\) est le nombre d’observations, et \\(k\\) est le nombre de variables indépendantes (1 pour la régression linéaire simple). Vous pouvez calculer approximativement la puissance avec G*Power pour une valeur de pente que vous jugez assez grande pour mériter d’être détectée. Allez à Tests: Means: One group: difference from constant, là, vous devrez remplacer la valeur de \\(b\\) dans l’équation pour la taille d’effet (d) par la pente que vous voudriez détecter, mais utiliser l’erreur type calculée à partir de vos données. Par exemple, supposons que les ichthyologues considèrent qu’une pente de 0.1 pour la relation entre log10(fklngth) et log10(age) est signifiante biologiquement, et qu’ils désirent estimer la puissance de détecter une telle pente à partir d’un échantillon de 20 esturgeons. Les résultats de la régression log-log nous fournissent ce dont on a besoin: summary(RegModel.2) ## ## Call: ## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.082794 -0.016837 -0.000719 0.021102 0.087446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.19199 0.02723 43.77 &lt;2e-16 *** ## log10(age) 0.34086 0.02168 15.72 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03015 on 73 degrees of freedom ## (5 observations deleted due to missingness) ## Multiple R-squared: 0.772, Adjusted R-squared: 0.7688 ## F-statistic: 247.1 on 1 and 73 DF, p-value: &lt; 2.2e-16 L’erreur-type de la pente est 0.02168. Il y avait 75 poissons (n=75) dans l’échantillon de départ. On peut donc calculer la métrique de taille d’effet pour G*Power \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} = \\frac{0.1}{0.02168\\sqrt{74-1-1}}=0.54\\] Armés de cette taille d’effet (une pente présumée de 0.1 et une variabilité autour de la régression similaire à la régression de fklngth vs age), aller à Tests: Means: One group: difference from constant, et entrez la valeur calculée de d, alpha, et l’effectif de l’échantillon pour calculer la puissance. Figure 3.3: Analyse de puissance pour N = 20 et pente = 0.1 La puissance de détecter une pente comme étant statistiquement significative (au niveau alpha), si la pente est 0.1, que la variabilité résiduelle autour de la régression est semblable à celle de notre échantillon (ce qui revient à une taille d’effet de 0.54, pour un échantillon de 20 esturgeons et alpha=0.05) est de 0.629. Seulement environ 2/3 des échantillons de cette taille détecteraient un effet significatif de l’âge sur fklngth. 3.9.2 Effectif requis pour atteindre une puissance désirée (test A-priori) Pour estimer la taille d’échantillon (effectif) requis pour avoir une puissance de 99% de détecter un effet de l’âge si la pente est 0.1 (sur une échelle log-log ), avec alpha=0.05, on utilise la même valeur de d (0.54): Figure 3.4: Analyse à priori pour déterminer la taille d’échantillon pour une puissance de 0.99 En augmentant la taille de l’échantillon à 65, selon le même scénario que précédemment, la puissance augmente à 99%. 3.10 Bootstrap en régression simple avec R Un test non paramétrique pour l’ordonnée à l’origine et la pente d’une régression simple peut être effectué par bootstrap. #charger le paquet boot library(boot) # obtenir les poids de régression bs &lt;- function(formula, data, indices) { d &lt;- data[indices,] # allows boot to select sample fit &lt;- lm(formula, data = d) return(coef(fit)) } # bootstrapping with 1000 replications results &lt;- boot( data = sturgeon.male, statistic = bs, R=1000, formula=log10(fklngth)~log10(age)) # view results results ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sturgeon.male, statistic = bs, R = 1000, formula = log10(fklngth) ~ ## log10(age)) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 1.1919926 0.001540598 0.03344248 ## t2* 0.3408557 -0.001102419 0.02647719 Pour chaque paramètre du modèle (ici l’ordonnée à l’origine est appelée t1* et la pente de la régression t2*), R imprime : original la valeur estimée sur tout l’échantillon bias la différence entre la valeur moyenne des estimés par bootstrap et la valeur originale sur tout l’échantillon std. error l’erreur-type de l’estimé bootstrap par(mfrow = c(2,2)) plot(results, index=1) # intercept plot(results, index=2) # log10(age) La distribution des estimés obtenus par bootstrap est assez normale dans cet exemple, avec de petites déviations dans les queuee de la distribution (là où ça compte pour les intervalles de confiance…). On pourrait utiliser l’erreur-type des estimés bootstrap pour calculer un intervalle de confiance symétrique (moyenne +- t ET). Cependant, comme R peut facilement calculer des intervalles de confiance qui corrigent pour le biais (BCa) ou encore des intervalle empiriques à partir des distributions simulées (méthode Percentile) il peut être aussi simple de les calculer selon les 3 méthodes: # interval de confiance pour l&#39;ordonnée à l&#39;origine boot.ci(results, type = &quot;all&quot;, index = 1) ## Warning in boot.ci(results, type = &quot;all&quot;, index = 1): bootstrap variances needed ## for studentized intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;all&quot;, index = 1) ## ## Intervals : ## Level Normal Basic ## 95% ( 1.125, 1.256 ) ( 1.128, 1.256 ) ## ## Level Percentile BCa ## 95% ( 1.128, 1.256 ) ( 1.112, 1.249 ) ## Calculations and Intervals on Original Scale #intervalle de confiance pour la pente boot.ci(results, type = &quot;all&quot;, index = 2) ## Warning in boot.ci(results, type = &quot;all&quot;, index = 2): bootstrap variances needed ## for studentized intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;all&quot;, index = 2) ## ## Intervals : ## Level Normal Basic ## 95% ( 0.2901, 0.3939 ) ( 0.2887, 0.3927 ) ## ## Level Percentile BCa ## 95% ( 0.2890, 0.3930 ) ( 0.2958, 0.3989 ) ## Calculations and Intervals on Original Scale Ici, les 4 types d’intervalles de confiance que R a calculé sont essentiellement semblables. Si les données avaient violé plus sévèrement les conditions d’application de la régression (normalité, homoscedasticité), alors les différentes méthodes (Normal, Basic, Percentile, et BCa) auraient divergé un peu plus. Lequel choisir alors? BCa est celui qui est préféré de la majorité des praticiens, présentement. "],
["comparaison-de-deux-échantillons.html", "4 Comparaison de deux échantillons 4.1 Paquets et données requises pour le labo 4.2 Examen visuel des données 4.3 Comparer les moyennes de deux échantillons indépendants 4.4 Bootstrap et tests de permutation pour comparer deux moyennes 4.5 Comparer les moyennes de deux échantillons appariés 4.6 Références", " 4 Comparaison de deux échantillons Après avoir complété cet exercice de laboratoire, vous devriez pouvoir: Utiliser R pour examiner visuellement vos données Utiliser R pour comparer les moyennes de deux échantillons tirés de populations normales Utiliser R pour comparer les moyennes de deux échantillons tirés de populations qui ne sont pas normales Utiliser R pour comparer les moyennes de deux échantillons appariés. 4.1 Paquets et données requises pour le labo Ce laboratoire nécessite: les paquets R: car lmtest boot lmPerm les fichiers de données sturgeon.csv skulldat.csv 4.2 Examen visuel des données Une des premières étapes dans toute analyse de données est l’examen visuel des données par des graphiques et statistiques sommaires pour détecter les distributions sous-jacentes, les valeurs extrêmes et les tendances dans vos données. Cela commence souvent avec des graphiques de vos données (histogrammes, diagrammes de probabilité, Box plots, etc.) qui vous permettent d’évaluer si vos données sont normales, si elles sont corrélées les unes aux autres, ou s’il y a des valeurs suspectes dans le fichier. Supposons que l’on veuille comparer la distribution en taille des esturgeons de The Pas et Cumberland House. La variable fklngth dans le fichier sturgeon.csv représente la longueur (en cm) à la fourche de chaque poisson mesurée de l’extrémité de la tête à la base de la fourche de la nageoire caudale. Pour commencer, examinons si cette variable est normalement distribuée. On ne va pas tester pour la normalité à ce stade-ci; la présomption de normalité dans les analyses paramétriques s’applique aux résidus et non aux données brutes. Cependant, si les données brutes ne sont pas normales, vous avez d’habitude une très bonne raison de soupçonner que les résidus vont aussi ne pas avoir une distribution normale. Une excellente façon de comparer visuellement une distribution à la distribution normale est de superposer un histogramme des données observées à une courbe normale. Pour ce faire, il faut procéder en deux étapes : indiquer à R que nous voulons créer un histogramme superposé à une courbe normale spécifier qu’on veut que les graphiques soient faits pour les deux sites En utilisant les données du fichier sturgeon.csv, générez les histogrammes et les approximations des distributions normales ajustées aux données de fklngth à The Pas et Cumberland House. # use &quot;sturgeon&quot; dataframe to make plot called mygraph # and define x axis as representing fklngth mygraph &lt;- ggplot( data = sturgeon, aes(x = fklngth)) + xlab(&quot;Fork length (cm)&quot;) # add data to the mygraph ggplot mygraph &lt;- mygraph + geom_density() + # add data density smooth geom_rug() + # add rug (bars at the bottom of the plot) geom_histogram( # add black semitransparent histogram aes(y = ..density..), color = &quot;black&quot;, alpha = 0.3) + # add normal curve in red, with mean and sd from fklength stat_function(fun = dnorm, args = list( mean = mean(sturgeon$fklngth), sd = sd(sturgeon$fklngth) ), color = &quot;red&quot;) #display graph, by location mygraph+facet_grid(.~location) Figure 4.1: Distribution de la longueur des esturgeons Examinez ce graphique et essayez de déterminer si ces deux échantillons sont normalement distribués. À mon avis, cette variable est approximativement normalement distribuée dans les deux échantillons. Puisque ce qui nous intéresse est de comparer la taille des poissons de deux sites différents, c’est probablement une bonne idée de créer un graphique qui compare les deux groupes de données. Un Box plot convient très bien pour cette tâche. Tracez un boxplot de fklngth groupé par location. Que concluez-vous quant à la différence entre les deux sites? ggplot(data=sturgeon, aes(x=location, y=fklngth))+geom_boxplot(notch=TRUE) Figure 4.2: Boxplot de la longueur des esturgeons Il n’y a pas de grande différence de taille entre les deux sites, mais la taille des poissons à The Pas est plus variable ayant une plus large étendue de taille et des valeurs extrêmes (définies par les valeurs qui sont &gt; 1.5*l’étendue interquartile) à chaque bout de la distribution. 4.3 Comparer les moyennes de deux échantillons indépendants Éprouvez l’hypothèse nulle d’égalité de la longueur à la fourche à The Pas et Cumberland House de 3 manières différentes: paramétriques supposant des variances égales paramétriques supposant des variances différentes non-paramétriques Que concluez-vous? # t-test assuming equal variances t.test( fklngth~location, data = sturgeon, alternative =&#39;two.sided&#39;, var.equal = TRUE) ## ## Two Sample t-test ## ## data: fklngth by location ## t = 2.1359, df = 184, p-value = 0.03401 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1308307 3.2982615 ## sample estimates: ## mean in group CUMBERLAND mean in group THE_PAS ## 45.08439 43.36984 # t-test assuming unequal variances t.test( fklngth ~ location, data = sturgeon, alternative =&#39;two.sided&#39;, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: fklngth by location ## t = 2.2201, df = 169.8, p-value = 0.02774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1900117 3.2390804 ## sample estimates: ## mean in group CUMBERLAND mean in group THE_PAS ## 45.08439 43.36984 # test non paramétrique wilcox.test( fklngth ~ location, data = sturgeon, alternative =&#39;two.sided&#39;) ## ## Wilcoxon rank sum test with continuity correction ## ## data: fklngth by location ## W = 4973, p-value = 0.06296 ## alternative hypothesis: true location shift is not equal to 0 En se fiant au test de t, on rejette donc l’hypothèse nulle. Il y a une différence significative entre les deux moyennes des longueurs à la fourche. Notez que si l’on se fie au test de Wilcoxon, il faut accepter l’hypothèse nulle. Les deux tests mènent donc à des conclusions contradictoires. La différence significative obtenue par le test de t peut provenir en partie d’une violation des conditions d’application du test (normalité et homoscédasticité). D’un autre côté, l’absence de différence significative selon le test de Wilcoxon pourrait être due au fait que, pour un effectif donné, la puissance du test non paramétrique est inférieure à celle du test paramétrique correspondant. Compte tenu 1) des valeurs de p obtenues pour les deux tests, et 2) le fait que pour des grands échantillons (des effectifs de 84 et 101 sont considérés grands) le test de t est considéré robuste, il est raisonnable de rejeter l’hypothèse nulle. Avant d’accepter les résultats du test de t et de rejeter l’hypothèse nulle qu’il n’y a pas de différences de taille entre les deux sites, il est important de déterminer si les données remplissent les conditions de normalité des résidus et d’égalité des variances. L’examen préliminaire suggérait que les données sont à peu près normales mais qu’il y avait peut-être des problèmes avec les variances (puisque l’étendue des données pour The Pas était beaucoup plus grande que celle pour Cumberland). On peut examiner ces conditions d’application plus en détail en examinant les résidus d’un modèle linéaire et en utilisant les graphiques diagnostiques: m1 &lt;- lm(fklngth ~ location, data=sturgeon) par(mfrow = c(2, 2)) plot(m1) Figure 4.3: Condition d’application du modèle linéaire Le premier graphique ci-dessus montre comment les résidus se distribuent autour des valeurs prédites (les moyennes) pour chaque site et permette de juger si il semble y avoir un problème de normalité ou d’homoscédasticité. Si les variances étaient égales dans les deux sites, l’étendue verticale des résidus tendrait à être la même. Sur le graphique, on voit que l’étendue des résidus est plus grande à gauche (le site où la taille moyenne est la plus faible), ce qui suggère un possible problème d’homogénéité des variances. On peut tester cela plus formellement en comparant la moyenne de la valeur absolue des résidus.(on y reviendra; c’est le test de Levene). Le second graphique est un graphique de probabilité (graphique Q-Q) des résidus. Comme ici, les points tombent près de la diagonale, il ne semble pas y avoir de problème important avec la normalité. On peut faire un test formel de la condition de normalité par le test de Shapiro- Wilk: shapiro.test(residuals(m1)) ## ## Shapiro-Wilk normality test ## ## data: residuals(m1) ## W = 0.97469, p-value = 0.001857 Hummm. Ce test indique que les résidus ne sont pas normaux, ce qui contredit notre évaluation visuelle. Cependant, puisque (a) la distribution des résidus ne s’éloigne pas beaucoup de la normalité et (b) le nombre d’observations à chaque site est raisonnablement grand (i.e. &gt;30), on n’a pas à être trop inquiet quant à l’impact de cette violation de normalité sur la fiabilité du test. Qu’en est-il de l’égalité des variances? leveneTest(m1) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 11.514 0.0008456 *** ## 184 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 bptest(m1) ## ## studentized Breusch-Pagan test ## ## data: m1 ## BP = 8.8015, df = 1, p-value = 0.00301 Les résultats qui précédents proviennent de 3 des tests disponibles en R (dans les package car et lmtest) qui éprouvent l’hypothèse de l’égalité des variances dans des tests de t ou des modèles linéaires ayant uniquement des variables indépendantes discontinues ou catégoriques. Il est redondant de faire les 2 tests. Si ils sont présentés ici, c’est que ces 2 tests sont usuels et qu’il n’y a pas consensus quant au meilleur des deux. Le test de Levene est le plus connu et utilisé. Il compare la moyenne des valeurs absolues des résidus dans les deux groupes. Le test Breusch-Pagan a l’avantage d’être applicable à une plus large gamme de modèles linéaires (il peut être utilisé également avec des variables indépendantes continues, comme en régression). Ici, les deux tests mènent à la même conclusion: la variance diffère entre les deux sites. Sur la base de ces résultats, on peut conclure qu’il y a évidence (mais faible) pour rejeter l’hypothèse nulle qu’il n’y a pas de différence dans la taille de poissons entre les deux sites. On a utilisé une modification du test de t pour tenir compte du fait que les variances ne sont pas égales et nous sommes satisfaits que la condition de normalité des résidus a été remplie. Alors, fklngth à Cumberland est plus grande que fklngth à The Pas. 4.4 Bootstrap et tests de permutation pour comparer deux moyennes 4.4.1 Bootstrap Le bootstrap et les tests de permutation peuvent être utilisés pour comparer les moyennes (ou d’autres statistiques). Le principe général est simple et peut être effectué de diverses façons. Ici j’utilise certains des outils disponibles et le fait qu’une comparaison de moyenne peut être représentée par un modèle linéaire. On pourra utiliser un programme similaire plus tard quand on ajustera des modèles plus complexes. library(boot) La première section sert à définir une fonction (ici appelée bs) qui extraie les coefficients d’un modèle ajusté : # function to obtain model coefficients for each iteration bs &lt;- function(formula, data, indices) { d &lt;- data[indices, ] fit &lt;- lm(formula, data = d) return(coef(fit)) } La deuxième section avec la commande boot() fait le gros du travail: on prend les données dans sturgeon, on les bootstrap \\(R = 1000\\) fois, et chaque fois on ajuste le modèle fklngth vs location et on garde les valeurs calculées par la fonction bs. # bootstrapping with 1000 replications results &lt;- boot(data = sturgeon, statistic = bs, R = 1000, formula = fklngth ~ location) # view results results ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sturgeon, statistic = bs, R = 1000, formula = fklngth ~ ## location) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 45.084391 0.03820258 0.4431830 ## t2* -1.714546 -0.02461105 0.7651465 On obtient les estimés originaux pour les deux coefficients du modèle: la moyenne pour le premier (alphabétiquement) site soit Cumberland, et la différence entre les deux moyennes à Cumberland et The Pas. C’est ce second paramètre, la différence entre les moyennes, qui nous intéresse. plot(results, index = 2) Figure 4.4: Normalité des estimés de la différence des moyennes par bootstrap # get 95% confidence intervals boot.ci(results, type = &quot;bca&quot;, index = 2) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;bca&quot;, index = 2) ## ## Intervals : ## Level BCa ## 95% (-3.051, -0.113 ) ## Calculations and Intervals on Original Scale Comme l’intervalle de confiance n’inclue pas 0, on conclue que les moyennes ne sont pas les mêmes. 4.4.2 Permutation Les tests de permutation pour les modèles linéaires peuvent être effectués à l’aide du package lmPerm: m1Perm &lt;- lmp( fklngth ~ location, data = sturgeon, perm = &quot;Prob&quot;) ## [1] &quot;Settings: unique SS &quot; La fonction lmp() fait tout le travail pour nous. Ici, cette fonction est effectuée avec l’option perm pour choisir la règle utilisée pour stopper les calculs. L’option Probs arrête les permutations quand la déviation standard estimée pour la p-valeur tombe sous un seuil déterminé. C’est l’une des nombreuses règles qui peuvent possiblement être utilisées pour ne faire les permutations que sur un sous-ensemble des permutations possibles (ce qui prendrait souvent trrrrrès longthemps). summary(m1Perm) ## ## Call: ## lmp(formula = fklngth ~ location, data = sturgeon, perm = &quot;Prob&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.40921 -3.75370 -0.08439 3.76598 23.48055 ## ## Coefficients: ## Estimate Iter Pr(Prob) ## location1 0.8573 5000 0.0086 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.454 on 184 degrees of freedom ## Multiple R-Squared: 0.02419, Adjusted R-squared: 0.01889 ## F-statistic: 4.562 on 1 and 184 DF, p-value: 0.03401 Ici, la règle a limité à 1117 permutations le calcul. Notez que ce nombre va varier à chaque fois que vous tournerez cet petit bout de code. Ce sont des résultats obtenus par permutations aléatoires, donc vous devez vous attendre à de la variabilité. . La p-valeur estimée pour H0 est 0.0824. La différence observée pour fklngth between entre les deux sites était plus grande que les valeurs permutées environ (1-0.0824=presque 92%) des 1117 permutations. Notez que 1117 permutations ce n’est pas un si grand nombre de permutations que ça, et donc les faibles valeurs de p ne sont pas très précises. Si vous voulez des valeurs précises de p, vous devrez faire plus de permutations.. Vous pouvez ajuster 2 paramètres: maxIter, le nombre maximal de permutations (défaut 5000) et Ca, le seuil de précision désiré qui arrête les permutations quand l’erreur- type de p est plus petite que Ca*p (défaut=0.1) Le reste est la sortie standard pour un modèle ajusté à des données, avec le test paramétrique. Ici, la p-valeur, présumant que toutes les conditions d’application sont remplies, est 0.34. 4.5 Comparer les moyennes de deux échantillons appariés Dans certaines expériences les mêmes individus sont mesurés deux fois, par exemple avant et après un traitement ou encore à deux moments au cours de leur développement. Les mesures obtenues lors de ces deux événements ne sont pas indépendantes, et des comparaisons de ces mesures appariées doivent être faites. Le fichier skulldat.csv contient des mesures de la partie inférieure du visage de jeunes filles d’Amérique du Nord prises à 5 ans, puis à 6 ans (données de Newman and Meredith, 1956). Pour débuter, éprouvons l’hypothèse que la largeur de la figure est la même à 5 ans et à 6 ans en assumant que les mesures viennent d’échantillons indépendants. skull &lt;- read.csv(&quot;data/skulldat.csv&quot;) t.test(width~age, data = skull, alternative=&#39;two.sided&#39;, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: width by age ## t = -1.7812, df = 27.93, p-value = 0.08576 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.43002624 0.03002624 ## sample estimates: ## mean in group 5 mean in group 6 ## 7.461333 7.661333 Maintenant, effectuons le test apparié qui est approprié: Que conclure? Com- ment les résultats diffèrent-ils de la première analyse? Pourquoi? t.test(width~age, data = skull, alternative=&#39;two.sided&#39;, paired = TRUE) ## ## Paired t-test ## ## data: width by age ## t = -19.72, df = 14, p-value = 1.301e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2217521 -0.1782479 ## sample estimates: ## mean of the differences ## -0.2 La première analyse a comme supposition que les deux échantillons de filles de 5 et 6 ans sont indépendants, alors que la deuxième analyse a comme supposition que la même fille a été mesurée deux fois, une fois à 5 ans, et la deuxième fois à 6 ans. Notez que, dans le premier cas, on accepte l’hypothèse nulle, mais que le test apparié rejette l’hypothèse nulle. Donc, le test qui est approprié (le test apparié) indique un effet très significatif de l’âge, mais le test inapproprié suggère que l’âge n’importe pas. C’est parce qu’il y a une très forte corrélation entre la largeur du visage à 5 et 6 ans: graphskull &lt;- ggplot(data = skull, aes(x = width5, y = width6)) + geom_point() + labs(x = &quot;Skull width at age 5&quot;, y = &quot;Skull width at age 6&quot;) + geom_smooth() + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) graphskull ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 4.5: Relation entre la taille de la tête à 5 et 6 ans Avec r = NA. En présence d’une si forte corrélation, l’erreur-type de la différence appariée de largeur du visage entre 5 et 6 ans est beaucoup plus petit que l’erreur-type de la différence entre la largeur moyenne à 5 ans et la largeur moyenne à 6 ans. Par conséquent, la statistique t associée est beaucoup plus élevée pour le test apparié, la puissance du test est plus grande, et la valeur de p plus petite. Répétez l’analyse en utilisant l’alternative nonparamétrique, le test Wil-coxon signed-rank. ( Que concluez-vous? wilcox.test(width~age, data = skull, alternative=&#39;two.sided&#39;, paired = TRUE) ## Warning in wilcox.test.default(x = c(7.33, 7.49, 7.27, 7.93, 7.56, 7.81, : ## cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: width by age ## V = 0, p-value = 0.0007193 ## alternative hypothesis: true location shift is not equal to 0 Donc on tire la même conclusion qu’avec le test de t apparié et conclue qu’il y a des différences significatives entre la taille des crânes de filles âgées de 5 et 6 ans (quelle surprise!). Mais, attendez une minute! On a utilisé des tests bilatéraux ici mais, compte tenu de s connaissances sur la croissance des enfants, une hypothèse unilatérale serait préférable. Ceci peut être accommodé en modifiant l’option “alternative”. On utilise l’hypothèse alternative pour décider entre “less” ou “greater”. Ici on s’attends que si il y a une différence, width5 va être inférieur à width6, donc on utiliserait “less”. t.test(width~age, data = skull, alternative=&#39;less&#39;, paired = TRUE) ## ## Paired t-test ## ## data: width by age ## t = -19.72, df = 14, p-value = 6.507e-12 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.1821371 ## sample estimates: ## mean of the differences ## -0.2 wilcox.test(width~age, data = skull, alternative=&#39;less&#39;, paired = TRUE) ## Warning in wilcox.test.default(x = c(7.33, 7.49, 7.27, 7.93, 7.56, 7.81, : ## cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: width by age ## V = 0, p-value = 0.0003597 ## alternative hypothesis: true location shift is less than 0 4.6 Références Bumpus, H.C. (1898) The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. Biological Lectures, Woods Hole Biology Laboratory, Woods Hole, 11 th Lecture: 209 - 226. Newman, K.J. and H.V. Meredith. (1956) Individual growth in skele- tal bigonial diameter during the childhood period from 5 to 11 years of age. Amer. J. Anat. 99: 157 - 187. "],
["anova-à-un-critère-de-classification.html", "5 ANOVA à un critère de classification 5.1 Paquets et données requises pour le labo 5.2 ANOVA à un critère de classification et comparaisons multiples 5.3 Transformations de données et ANOVA non-paramétrique 5.4 Examen des valeurs extrêmes 5.5 Test de permutation", " 5 ANOVA à un critère de classification Après avoir complété cet exercice de laboratoire, vous devriez être capable de : Utiliser R pour effectuer une analyse de variance paramétrique à un critère de classification, suivie de comparaisons multiples * Utiliser R pour vérifier si les conditions d’application de l’ANOVA paramétrique sont remplies * Utiliser R pour faire une ANOVA à un critère de classification non-paramétrique * Utiliser R pour transformer des données de manière à mieux rem- plir les conditions d’application de l’ANOVA paramétrique. 5.1 Paquets et données requises pour le labo Ce laboratoire nécessite: les paquets R: multicomp car les fichiers de données Dam10dat.csv 5.2 ANOVA à un critère de classification et comparaisons multiples L’ANOVA à un critère de classification est l’analogue du test de t pour des comparaisons de moyennes de plus de deux échantillons. Les conditions d’application du test sont essentiellement les mêmes, et lorsque appliqué à deux échantillons ce test est mathématiquement équivalent au test de t. En 1961-1962, le barrage Grand Rapids était construit sur la rivière Saskatchewan en amont de Cumberland House. On croit que durant la construction plusieurs gros esturgeons restèrent prisonniers dans des sections peu profondes et moururent. Des inventaires de la population d’esturgeons furent faits en 1954, 1958, 1965 et 1966. Au cours de ces inventaires, la longueur à la fourche ( frklngth ) et la masse ( rndwght ) furent mesurées (pas nécessairement sur chaque poisson cependant). Ces données sont dans le fichier Dam10dat.csv. 5.2.1 Visualiser les données À partir des données, vous devez d’abord changer le type de donnée de la variable year, pour que R traite year comme une variable discontinue (factor) plutôt que continue. Dam10dat &lt;- read.csv(&quot;data/Dam10dat.csv&quot;) Dam10dat$year &lt;- as.factor(Dam10dat$year) str(Dam10dat) ## &#39;data.frame&#39;: 118 obs. of 21 variables: ## $ year : Factor w/ 4 levels &quot;1954&quot;,&quot;1958&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ fklngth : num 45 50 39 46 54.5 49 42.5 49 56 54 ... ## $ totlngth: num 49 NA 43 50.5 NA 51.7 45.5 52 60.2 58.5 ... ## $ drlngth : logi NA NA NA NA NA NA ... ## $ drwght : num 16 20.5 10 17.5 19.7 21.3 9.5 23.7 31 27.3 ... ## $ rdwght : num 24.5 33 15.5 28.5 32.5 35.5 15.3 40.5 51.5 43 ... ## $ sex : int 1 1 1 2 1 2 1 1 1 1 ... ## $ age : int 24 33 17 31 37 44 23 34 33 47 ... ## $ lfkl : num 1.65 1.7 1.59 1.66 1.74 ... ## $ ltotl : num 1.69 NA 1.63 1.7 NA ... ## $ ldrl : logi NA NA NA NA NA NA ... ## $ ldrwght : num 1.2 1.31 1 1.24 1.29 ... ## $ lrdwght : num 1.39 1.52 1.19 1.45 1.51 ... ## $ lage : num 1.38 1.52 1.23 1.49 1.57 ... ## $ rage : int 4 6 3 6 7 7 4 6 6 7 ... ## $ ryear : int 1954 1954 1954 1954 1954 1954 1954 1954 1954 1954 ... ## $ ryear2 : int 1958 1958 1958 1958 1958 1958 1958 1958 1958 1958 ... ## $ ryear3 : int 1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ... ## $ location: int 1 1 1 1 1 1 1 1 1 1 ... ## $ girth : logi NA NA NA NA NA NA ... ## $ lgirth : logi NA NA NA NA NA NA ... Ensuite, visualisez les données comme dans le labo pour les tests de t. Créez un histogramme avec ligne de densité, un diagramme de probabilité, et un Box plot par année. Que vous révèlent ces données? mygraph &lt;- ggplot(Dam10dat, aes(x = fklngth)) + labs (x = &quot;Fork length (cm)&quot;) + geom_density() + geom_rug() + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, alpha = 0.3) + stat_function(fun = dnorm, args = list( mean = mean(Dam10dat$fklngth), sd = sd(Dam10dat$fklngth)), color = &quot;red&quot;) #display graph, by year mygraph+facet_wrap(~year,ncol=2) Figure 5.1: Distribution de la longueur des esturgeons par année Il semble que la taille des esturgeons est un peu plus petite après la construction du barrage, mais les données sont très variables et les effets ne sont pas parfaitement clairs. Il y a peut-être des problèmes de normalité avec les échantillons de 1954 et 1966, et il y a probablement des valeurs extrêmes dans les échantillons de 1958 et 1966. On va continuer en testant les conditions d’application de l’ANOVA. Il faut d’abord faire l’analyse et examiner les résidus. 5.2.2 Vérifier les conditions d’application de l’ANOVA paramétrique L’ANOVA paramétrique a trois conditions principales d’application : 1) les résidus sont normalement distribués, 2) la variance des résidus est égale dans tous les traitements (homoscédasticité) et 3) les résidus sont indépendants les uns des autres. Ces conditions doivent être remplies avant qu’on puisse se fier aux résultats de l’ANOVA paramétrique. Faites une ANOVA à un critère de classification sur fklngth par année et produisez les graphiques diagnostiques # Fit anova model and plot residual diagnostics anova.model1 &lt;- lm(fklngth ~ year, data=Dam10dat) par(mfrow = c(2, 2)) plot(anova.model1) Figure 5.2: Conditions d’applications de l’ANOVA D’après les graphiques, on peut douter de la normalité et de l’homogénéité des variances. Notez qu’il y a un point qui ressort vraiment avec une forte valeur résiduelle (cas numéro 59) et qu’il ne s’aligne pas bien avec les autres valeurs: c’est la valeur extrême qui avait été détectée plus tôt. Ce point fera sans doute gonfler la variance résiduelle du groupe auquel il appartient. Des tests formels nous confirmeront ou infirmeront nos conclusions faites à partir de ces graphiques. Faites un test de normalité sur les résidus de l’ANOVA. shapiro.test(residuals(anova.model1)) ## ## Shapiro-Wilk normality test ## ## data: residuals(anova.model1) ## W = 0.91571, p-value = 1.63e-06 Ce test confirme nos soupçons: les résidus ne sont pas distribués normalement. Il faut cependant garder à l’esprit que la puissance est grande et que même de petites déviations de la normalité sont suffisantes pour rejeter l’hypothèse nulle. Ensuite, éprouvez l’hypothèse d’égalité des variances (homoscedasticité): leveneTest(fklngth ~ year, data=Dam10dat) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 2.8159 0.04234 * ## 114 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La valeur de p vous dit que vous pouvez rejeter l’hypothèse nulle qu’il n’y a aucune différence dans les variances entre les années. Alors, nous concluons que les variances ne sont pas homogènes. 5.2.3 Faire l’ANOVA Faites une ANOVA de fklnght en choisissant / en présumant pour l’instant que les conditions d’application sont suffisamment remplies. Que concluez-vous? summary(anova.model1) ## ## Call: ## lm(formula = fklngth ~ year, data = Dam10dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.2116 -2.6866 -0.7116 2.2103 26.7885 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.0243 0.8566 56.061 &lt; 2e-16 *** ## year1958 0.1872 1.3335 0.140 0.88859 ## year1965 -5.5077 1.7310 -3.182 0.00189 ** ## year1966 -3.3127 1.1684 -2.835 0.00542 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.211 on 114 degrees of freedom ## Multiple R-squared: 0.1355, Adjusted R-squared: 0.1128 ## F-statistic: 5.957 on 3 and 114 DF, p-value: 0.0008246 Les 4 coefficients peuvent être utilisés pour obtenir les valeurs prédites par le modèle (i.e. les moyennes de chaque groupe). La fklngth moyenne de la première année (1954) est 48.0243. Les coefficients pour les 3 autres années sont la différence entre la moyenne de l’année en question et la moyenne de 1954. La moyenne pour 1965 est 48.0243-5.5077=42.5166. Pour chaque coefficient, on a également accès à l’erreur-type, une valeur de t et la probabilité qui lui est associée (H0 que le coefficient est 0). Les poissons étaient plus petits après la construction du barrage qu’en 1954. Vous devez prendre ces p-valeurs avec un grain de sel, car elles ne sont pas corrigées pour les comparaisons multiples et. En général, je porte peu d’attention à cette partie des résultats imprimés et me concentre sur ce qui suit. La racine carrée de la variance des résidus (valeurs observées moins valeurs prédites) qui correspond à la variabilité inexpliquée par le modèle (variation de la taille des poissons capturés la même année). Le R-carré est la proportion de la variabilité de la variable dépendante qui peut être expliquée par le modèle. Ici, le modèle explique 13.5% de la variabilité. Les différences de taille d’une année à l’autre sont relativement petites lorsqu’on les compare à la variation de taille entre les poissons capturés la même année. La p-valeur associée au test “omnibus” que toutes les moyennes sont égales. Ici, p est beaucoup plus petit que 0.05 et on rejetterait H0 pour conclure que fklngth varie selon les années. La commande anova() produit le tableau d’ANOVA standard qui contient la plupart de cette information: anova(anova.model1) ## Analysis of Variance Table ## ## Response: fklngth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## year 3 485.26 161.755 5.9574 0.0008246 *** ## Residuals 114 3095.30 27.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La variabilité totale de fklngth, mesurée par la somme des carrés des écarts (Sum sq) est partitionnée en ce qui peut être expliqué par l’année (485.26) et la variabilité résiduelle inexpliquée (3095.30). L’année explique bien (485.26/(3095.30+485.26)=.1355 or 13.55% de la variabilité). Le carré moyen des résidus (Residual Mean Sq) est leur variance. 5.2.4 Les comparaisons multiples La fonction pairwise.t.test() peut être utilisée pour comparer des moyennes et ajuster (ou non, si désiré) les probabilités pour le nombre de comparaisons en utilisant l’une des options pour p.adj: Compare toutes les moyennes sans ajuster les probabilités pairwise.t.test(Dam10dat$fklngth, Dam10dat$year, p.adj = &quot;none&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Dam10dat$fklngth and Dam10dat$year ## ## 1954 1958 1965 ## 1958 0.8886 - - ## 1965 0.0019 0.0022 - ## 1966 0.0054 0.0079 0.1996 ## ## P value adjustment method: none Option bonf ajuste les p-valeurs avec la correction de Bonferroni. Ici, il y a 6 p-valeurs calculées, et la correction de Bonferroni revient à simplement multiplier la p-valeur par 6 (sauf si le résultat est supérieur à 1. Si tel est le cas, la p-valeur ajustée est 1). pairwise.t.test(Dam10dat$fklngth, Dam10dat$year, p.adj = &quot;bonf&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Dam10dat$fklngth and Dam10dat$year ## ## 1954 1958 1965 ## 1958 1.000 - - ## 1965 0.011 0.013 - ## 1966 0.033 0.047 1.000 ## ## P value adjustment method: bonferroni Option “holm” is est la correction séquentielle de Bonferroni dans laquelle les p-valeurs sont ordonnées de (i=1) la plus faible à (N) la plus grande. La correction pour les p-valeurs est (N-i+1). Ici, il y a N=6 paires de moyennes qui sont comparées. La plus petite valeur de p non corrigée est 0.0019 pour 1954 vs 1965. La p-valeur corrigée est donc \\(0.0019*(6-1+1)=0.011\\). La seconde plus petite p-valeur est 0.0022. Sa p-valeur corrigée est 0.0022*(6-2+1)=0.011. Pour la p- valeur la plus élevée, la correction est (N-N+1)=1, donc la p-valeur corrigée est égale à la p-valeur brute. pairwise.t.test(Dam10dat$fklngth, Dam10dat$year, p.adj = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Dam10dat$fklngth and Dam10dat$year ## ## 1954 1958 1965 ## 1958 0.889 - - ## 1965 0.011 0.011 - ## 1966 0.022 0.024 0.399 ## ## P value adjustment method: holm L’option “fdr” sert à contrôler le “false discovery rate”. pairwise.t.test(Dam10dat$fklngth, Dam10dat$year, p.adj = &quot;fdr&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Dam10dat$fklngth and Dam10dat$year ## ## 1954 1958 1965 ## 1958 0.8886 - - ## 1965 0.0066 0.0066 - ## 1966 0.0108 0.0119 0.2395 ## ## P value adjustment method: fdr Les quatre méthodes mènent ici à la même conclusion: les poissons sont plus gros après la construction du barrage et toutes les comparaisons entre les années 50 et 60 sont significatives alors que les différences entre 54 et 58 ou 65 et 66 ne le sont pas. La conclusion ne dépend pas du choix de méthode. Dans d’autres situations, vous pourriez obtenir des résultats contradictoires. Alors, quelle méthode choisir? Les p-valeurs qui ne sont pas corrigées sont certainement suspectes lorsqu’il y a plusieurs comparaisons. D’un autre coté, la correction de Bonferroni est conservatrice et le devient encore plus lorsqu’il y a de très nombreuses comparaisons. Des travaux récents suggèrent que la correction fdr est un bon compromis lorsqu’il y a beaucoup de comparaisons. La méthode de Tukey est l’une des plus populaires et est facile à utiliser en R (notez cependant qu’il y a une sale petit bogue qui se manifeste quand la variable indépendante peut ressemble à un nombre plutôt qu’un facteur, ce qui explique la petite pirouette avec paste dans mon code): Dam10dat$myyear &lt;- as.factor(paste(&quot;&quot;, Dam10dat$year)) TukeyHSD(aov(fklngth ~ myyear, data = Dam10dat)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = fklngth ~ myyear, data = Dam10dat) ## ## $myyear ## diff lwr upr p adj ## 1958- 1954 0.1872141 -3.289570 3.6639986 0.9990071 ## 1965- 1954 -5.5076577 -10.021034 -0.9942809 0.0100528 ## 1966- 1954 -3.3126964 -6.359223 -0.2661701 0.0274077 ## 1965- 1958 -5.6948718 -10.436304 -0.9534397 0.0116943 ## 1966- 1958 -3.4999106 -6.875104 -0.1247171 0.0390011 ## 1966- 1965 2.1949612 -2.240630 6.6305526 0.5710111 plot(TukeyHSD(aov(fklngth ~ myyear, data = Dam10dat))) Figure 5.3: Différence anuelles dans la longueur des esturgeons Les intervalles de confiance, corrigés pour les comparaisons multiples par la méthode de Tukey, sont illustrés pour les différences entre années. Malheureusement les légendes ne sont pas complètes, mais l’ordre est le même que dans le tableau précédent. Le package multcomp peut produire de meilleurs graphiques, mais requiert un peu plus de code: # Alternative way to compute Tukey multiple comparisons # set up a one-way ANOVA anova.fkl.vs.year &lt;- aov(aov(fklngth ~ myyear, data = Dam10dat)) # set up all-pairs comparisons for factor `year&#39; meandiff &lt;- glht(anova.fkl.vs.year, linfct = mcp(myyear = &quot;Tukey&quot;)) confint(meandiff) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = aov(fklngth ~ myyear, data = Dam10dat)) ## ## Quantile = 2.5938 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## 1958 - 1954 == 0 0.1872 -3.2715 3.6459 ## 1965 - 1954 == 0 -5.5077 -9.9976 -1.0177 ## 1966 - 1954 == 0 -3.3127 -6.3434 -0.2820 ## 1965 - 1958 == 0 -5.6949 -10.4117 -0.9781 ## 1966 - 1958 == 0 -3.4999 -6.8576 -0.1423 ## 1966 - 1965 == 0 2.1950 -2.2176 6.6075 old.par &lt;- par(mai = c(1, 1.25, 1, 1)) plot(meandiff) Figure 5.4: Différence anuelles dans la longueur des esturgeons par(old.par) C’est un peu mieux, mais ce qui le serait encore plus c’est un graphique des moyennes, avec leurs intervalles de confiance ajustés pour les comparaisons multiples: # Compute and plot means and Tukey CI means &lt;- glht(anova.fkl.vs.year, linfct = mcp(myyear = &quot;Tukey&quot;)) cimeans &lt;- cld(means) # use sufficiently large upper margin old.par &lt;- par(mai = c(1, 1, 1.25, 1)) # plot plot(cimeans) Figure 5.5: Différence anuelles dans la longueur des esturgeons par(old.par) Notez les lettres au dessus du graphique: les années étiquetées avec la même lettre ne diffèrent pas significativement l’une de l’autre. 5.3 Transformations de données et ANOVA non-paramétrique Dans l’exemple précédent sur les différences annuelles de la variable fklgnth, on a noté que les conditions d’application de l’ANOVA n’étaient pas remplies. Si les données ne remplissent pas les conditions de l’ANOVA paramétrique, il y a 3 options : 1) Ne rien faire. Si les effectifs dans chaque groupe sont grands, on peut relaxer les conditions d’application car l’ANOVA est alors assez robuste aux violations de normalité (mais moins aux violations d’homoscedasticité), 2) on peut transformer les données, ou 3) on peut faire une analyse non-paramétrique. Refaites l’ANOVA de la section précédente après avoir transformé en faisant le logarithme à la base de 10. Avec les données transformées, est-ce que les problèmes qui avaient été identifiés dis- paraissent ? # Fit anova model on log10 of fklngth and plot residual diagnostics par(mfrow = c(2, 2)) anova.model2 &lt;- lm(log10(fklngth) ~ year, data=Dam10dat) plot(anova.model2) Figure 5.6: Conditions d’application de l’ANOVA Les graphiques diagnostiques des résidus donnent: Les graphiques sont à peine mieux ici. Si on fait le test Wilk-Shapiro sur les résidus, on obtient: shapiro.test(residuals(anova.model2)) ## ## Shapiro-Wilk normality test ## ## data: residuals(anova.model2) ## W = 0.96199, p-value = 0.002048 Alors, on a toujours des problèmes avec la normalité et on est juste sur le seuil de décision pour l’égalité des variances. Vous avez le choix à ce point: 1) essayer de trouver une autre transformation pour mieux rencontrer les conditions d’application, 2) assumer que les données sont rencontrent suffisamment les conditions d’application, ou 3) faire une ANOVA non-paramétrique. L’analogue non-paramétrique de l’ANOVA à un critère de classifica- tion le plus employé est le test de Kruskall-Wallis. Faites ce test sur fklngth et comparez les résultats à ceux de l’analyse paramétrique. Que concluez-vous? kruskal.test(fklngth ~ year, data=Dam10dat) ## ## Kruskal-Wallis rank sum test ## ## data: fklngth by year ## Kruskal-Wallis chi-squared = 15.731, df = 3, p-value = 0.001288 La conclusion est donc la même qu’avec l’ANOVA paramétrique: on rejette l’hypothèse nulle que le rang moyen est le même pour chaque année. Donc, même si les conditions d’application de l’analyse paramétrique n’étaient pas parfaitement rencontrées, les conclusions sont les mêmes, ce qui illustre la robustesse de l’ANOVA paramétrique. 5.4 Examen des valeurs extrêmes Vous devriez avoir remarqué au cours des analyses précédentes qu’il y avait peut-être des valeurs extrêmes dans les données. Ces points étaient évidents dans le Box Plot de fklngth by year et ont été notés comme les points 59, 23, et 87 dans les diagrammes de probabilité des résidus et dans le diagramme de dispersion des résidus et des valeurs estimées. En général, vous devez avoir de très bonnes raisons pour enlever des valeurs extrêmes de la base de données (i.e. vous savez qu’il y a eu une erreur avec un cas). Cependant, il est quand même toujours valable de voir comment l’analyse change en enlevant des valeurs extrêmes de la base de données. Répétez l’ANOVA originale sur fklngth et year mais faites le avec un sous-ensemble de données sans les valeurs extrêmes. Est-ce que les conclusions ont changé? Damsubset&lt;-Dam10dat[-c(23,59,87),] #removes obs 23, 59 and 87 aov.Damsubset &lt;- aov(fklngth ~ as.factor(year), Damsubset) summary(aov.Damsubset) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(year) 3 367.5 122.50 6.894 0.000267 *** ## Residuals 111 1972.4 17.77 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 shapiro.test(residuals(aov.Damsubset)) ## ## Shapiro-Wilk normality test ## ## data: residuals(aov.Damsubset) ## W = 0.98533, p-value = 0.2448 leveneTest(fklngth ~ year, Damsubset) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 4.6237 0.004367 ** ## 111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’élimination de trois valeurs extrêmes améliore un peu les choses, mais ce n’est pas parfait. On a toujours une problème avec les variances, mais les résidus sont maintenant normaux. Cependant, le fait que la conclusion qu’on tire de l’ANOVA originale ne change pas en enlevant les points renforce le fait qu’on n’a pas une bonne raison pour enlever les points. Commandes R pour refaire l’ANOVA sur le sous-ensemble de données Damsubset&lt;-Dam10dat[-c(23,59,87),] # removes obs 23, 59 and 87 aov.Damsubset &lt;- aov(fklngth ~ as.factor(year), Damsubset) summary(aov.Damsubset) shapiro.test(residuals(aov.Damsubset)) leveneTest(fklngth ~ year, Damsubset) 5.5 Test de permutation Commande R pour un test de permutation d’une ANOVA à un critère de classification. ############################################################# # Permutation Test for one-way ANOVA # modified from code written by David C. Howell # http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html # set desired number of permutations nreps &lt;-500 # to simplify reuse of this code, copy desired dataframe to mydata mydata&lt;-Dam10dat # copy model formula to myformula myformula&lt;-as.formula(&quot;fklngth ~ year&quot;) # copy dependent variable vector to mydep mydep&lt;-mydata$fklngth # copy independent variable vector to myindep myindep&lt;-as.factor(mydata$year) ################################################ # You should not need to modify code chunk below ################################################ # Compute observed F value for original sample mod1 &lt;- lm(myformula, data=mydata) # Standard Anova ANOVA &lt;- summary(aov(mod1)) # Save summary to variable observedF&lt;- ANOVA[[1]]$&quot;F value&quot;[1] # Save observed F value # Print standard ANOVA results cat(&quot; The standard ANOVA for these data follows &quot;, &quot;\\n&quot;) print(ANOVA, &quot;\\n&quot;) cat(&quot;\\n&quot;) cat(&quot;\\n&quot;) print(&quot;Resampling as in Manly with unrestricted sampling of obser- vations. &quot;) # Now start resampling Fboot &lt;- numeric(nreps) # initalize vector to receive permuted values Fboot[1] &lt;- observedF for (i in 2:nreps) { newdependent &lt;- sample(mydep, length(mydep)) # randomize dep var mod2 &lt;- lm(newdependent ~ myindep) # refit model b &lt;- summary(aov(mod2)) Fboot[i] &lt;- b[[1]]$&quot;F value&quot;[1] # store F stats } permprob &lt;- length(Fboot[Fboot &gt;= observedF])/nreps cat(&quot; The permutation probability value is: &quot;, permprob, &quot;\\n&quot;) # end of code chunk for permutation Version lmPerm du test de permutation. ## lmPerm version of permutation test require(lmPerm2) # for generality, copy desired dataframe to mydata # and model formula to myformula mydata &lt;- Dam10dat myformula &lt;- as.formula(&quot;fklngth ~ year&quot;) # Fit desired model on the desired dataframe mymodel &lt;- lm(myformula, data = mydata) # Calculate permutation p-value anova(lmp(myformula, data = mydata, perm = &quot;Prob&quot;, center=FALSE, Ca=0.001)) "],
["petit-guide-pour-rmarkdown.html", "A Petit guide pour Rmarkdown A.1 Syntaxe Markdown A.2 À propos de LaTex A.3 Concernant le code dans notebook", " A Petit guide pour Rmarkdown A.1 Syntaxe Markdown Ceci est un petit document qui résume la syntaxe de Markdown On utilise # (6 max.) pour créer des titres (section) : ### Titre 3 #### Titre 4 #### Titre 5 ##### Titre 6 Donne A.1.1 Titre 3 A.1.1.1 Titre 4 A.1.1.2 Titre 5 A.1.1.2.1 Titre 6 On entoure le texte des symboles * ou _ pour mettre le texte en italique ou en gras. Un symbole pour mettre en italique: *Texte en Italique* Texte en Italique _Texte en Italique_ Texte en Italique Deux symboles pour mettre en gras: **Texte gras** Texte gras __Texte gras__ Texte gras On utilise une combinaison des 2 charactère pour obtenir du texte gras et italique. Pour créer un liste on utilise le symbole - suivit d’une espace. Il doit y avoir une ligne vide avant la liste. On doit ajouter quelques espaces après chaque ligne. Pour ajouter des niveau à la liste on indente (tab) 2 fois, puis on ajoute le symbole + suivit d’un espace. Ici encore, on doit ajouter quelques espaces après chaque lignes. Il doit y avoir une ligne vide après la liste. On peut aussi créer des liste avec des numéro ou des lettre. On doit simplement mettre un point après le chiffre ou la lettre, suivit d’un espace Encore une fois, il peut y avoir plusieurs niveaux ii). Mais pas plus de 3 niveau Pour écrire des équation mathématique, on entoure le texte du symbole $ \\(K = 0.5\\) \\(\\log(x + k)\\) Pour faire un tableau, on fait comme suit (noter que l’alignement des symbole est sans importance) : colonne 1 |colonne 2|colonne 3|colonne 4 ---------|--------|--------|----------- cellulle | cellulle |cellulle| cellulle cellulle | cellulle |cellulle| cellulle cellulle | cellulle |cellulle| cellulle On peut utiliser le symbole &gt; pour indenter un paragraphe (le résultat est différent entre PDF et HTML) : En HTML, ça produit un résultat étrange … &gt; mais bon … On peut écrire dans la même police que celle du code en entourant le texte avec le symbole ` : variable Si l’on veut utilisé un symbole dans le texte sans qu’il modifie le texte, il faut mettre le symbole \\ devant. Pour inséré un lien, on entoure le texte avec [ ], et on met l’addresse dans des paranthèse () immédiatement à coté: R Markdown On peut tracer un ligne horizontale en répétant le symbole - 3 fois ou plus : A.2 À propos de LaTex Ce qui suit concerne uniquement ceux qui on installer LaTex. Les fonction LaTex serve à modifier le texte, mais ne fonctionne que lorsqu’on produit un document PDF (ça fonctionne peut-être aussi pour les document word). Ils en existe beaucoup trop de fonction pour toutes les énumérer ici. Google est votre ami. Voici quelques exemple: \\emph{texte italique} \\textcolor{red}{texte rouge} \\texttt{même police que le code de R} Le nom de la prochaine fonction est assez descriptif \\pagebreak A.3 Concernant le code dans notebook Vous devez écrire le code à l’intérieur de bloc de code, sinon il ne sera pas interpréter comme du code, mais comme du texte. Vous pouvez taper directement la notation pour créer un bloc code ou utiliser les raccourci clavier de RStudio Pour créer un nouveau bloc de code, taper ```{r} pour ouvrir le bloc et ``` pour le fermer. Sinon, appuyer sur Ctrl+Alt+I Dans Rstudio, pour éxécuter le code à l’intérieur d’un bloc appuyer sur Ctrl+Shift+Enter ou cliquer sur le boutton Run (dans le bloc, en haut à droite). Charger les données et les packages dans le premier bloc de code. ```{r} #ouverture du bloc de code code ``` #fermeture du bloc code Lorsque vous exécuter du code dans un bloc, le résultat apparait directement en dessous de la du bloc (si votre document est un R Notebook). L’option fig.cap fonctionnement seulement lorsqu’on génère un PDF. Lorsque vous sauvegarder un notebook, un fichier HTLM contenant le code et les résultats est sauvegarder en même temps. Cliquer sur le bouton Preview ou appuyer sur Ctrl+Shift+K pour voir le fichier HTML. ```{r, fig.cap = &quot;Exemple de graphique tiré du fichier d&#39;aide de la fonction stat_bin&quot;} ggplot(diamonds, aes(carat)) + geom_histogram() ggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.01) ggplot(diamonds, aes(carat)) + geom_histogram(bins = 200) ``` ggplot(diamonds, aes(carat)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure A.1: Exemple de graphique tiré du fichier d’aide de la fonction stat_bin ggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.01) Figure A.2: Exemple de graphique tiré du fichier d’aide de la fonction stat_bin ggplot(diamonds, aes(carat)) + geom_histogram(bins = 200) Figure A.3: Exemple de graphique tiré du fichier d’aide de la fonction stat_bin "]
]
